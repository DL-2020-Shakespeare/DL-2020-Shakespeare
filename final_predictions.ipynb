{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import random as rn\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from importlib import reload\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from IPython.display import display\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, ndcg_score, \\\n",
    "        label_ranking_average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import data\n",
    "import models\n",
    "import preprocessing\n",
    "\n",
    "seed = 42\n",
    "sns.set()\n",
    "\n",
    "def reset_seed():\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose notebook version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"tokenized_cased\"\n",
    "# version = \"tokenized_no_sw_no_punct\"\n",
    "# version = \"tokenized_lemmatized_no_sw_no_punct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299773,)\n",
      "(299773, 126)\n",
      "Toronto stocks end higher after volatile session. CHANGE\t\t\t\t    CHANGE TSE\t  5900.37    +50.15   HI 5900.37\t    LO  5840.29 DJI\t  6611.05    +27.57   GOLD (LONDON)   US$350.00 +1.90 FTSE100    4248.10    -64.80   GOLD (NY-COMEX) US$354.80 +0.70 NIKKEI    17869.59   -133.81   LME CASH NICKEL US$7659   +99.0 CANDLR\t1.3883\t\t LME CASH ALUM   US$1602.0  -4.0 CAN 30-YR   107.41     -0.15   BRENT CRUDE     US$19.09  -0.27 --------------------MARKET COMMENT---------------------------- * Toronto stocks ended higher on Tuesday, buoyed by strength in golds and banking * Computer problems due to heavy trading in Bre-X Minerals hampered session * 84 million shares traded Toronto's key stock index ended higher on Tuesday as the saga of Bre-X Minerals Ltd and its Indonesian gold find continued to dominate Canada's biggest stock market. The TSE 300 Index climbed 50.15 points to close at 5900.37 in heavy turnover of 84.07 million shares worth C$1.4 billion. But the overall market was mixed with declining issues narrowly outpacing advances 476 to 464. 298 issues were flat. Frantic trading in Bre-X collapsed the TSE's computer trading system earlier in the day, forcing the exchange to halt trading in the stock before the market closed. Shares in the Calgary-based gold prospector were halted for a statement by the company this morning. When it resumed, a whopping 7.7 million shares changed hands in the first 22 minutes of trading before the system crashed. Bre-X closed up 1.35 at 3.85. It was the first time Bre-X traded since investors lopped nearly C$3 billion off its stock market value last Thursday. TSE officials said the trading problems were due to old technology which will be replaced. On the Montreal Exchange, Bre-X closed up 0.81 at 3.50 on 9.8 million shares. Analysts predicted more volatility for Bre-X shares this week. \"The question of what Bre-X will release over the next few days will be important to the market,\" said Josef Schachter, of Schachter Asset Management Inc. The gold sector rose nearly 136 points, leading 12 of 14 sub-indices higher. Other strong groups included financial services, consumer products, energy and transportation. The TSE posted minor losses in forestry and real estate. --- HOT STOCKS --- * Among bank shares, Bank of Nova Scotia rose 0.65 to 51.50 on 2.1 million shares, while Canadian Imperial Bank of Commerce added 0.50 to 31.80 on 2.1 million shares. ((Reuters Toronto Bureau (416) 941-8100))\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# data.extract_data(extraction_dir=\"train\",\n",
    "#                   data_dir=\"data\",\n",
    "#                   data_zip_name=\"reuters-training-corpus.zip\")\n",
    "\n",
    "train_df = pd.read_pickle(\"train/data.pkl\")\n",
    "\n",
    "# train_df = data.get_docs_labels(\"train/REUTERS_CORPUS_2\")\n",
    "# train_df.to_pickle(\"train/data.pkl\")\n",
    "\n",
    "train_docs = train_df[\"doc\"].values\n",
    "n_train = train_docs.shape[0]\n",
    "train_labels = np.array(train_df[\"labels\"].tolist())\n",
    "n_labels = len(data.CODEMAP)\n",
    "\n",
    "print(train_docs.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_docs[2])\n",
    "print(train_labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33142,)\n",
      "OFFICIAL JOURNAL CONTENTS - OJ L 94 OF APRIL 9, 1997. * (Note - contents are displayed in reverse order to that in the printed Journal) * COMMISSION DECISION of 14 March 1997 amending the list of declining industrial areas concerned by Objective 2 as defined by Council Regulation (EEC) No 2052/88 (97/237/EC) COMMISSION DECISION of 13 March 1997 authorizing the grant by Finland of national aid in the reindeer sector (Only the Finnish and Swedish texts are authentic) (97/236/EC) COMMISSION DECISION of 6 March 1997 approving a modification to the single programming document for Community structural measures for improving the processing and marketing conditions for agricultural products in the Grand Duchy of Luxembourg, in respect of Objective 5 (a), covering the period between 1994 and 1999 (Only the French text is authentic) (97/235/EC) COMMISSION DECISION of 3 March 1997 amending Decision 96/233/EC establishing the list of approved fish farms in Denmark (Text with EEA relevance) (97/234/EC) COMMISSION REGULATION (EC) No 619/97 of 8 April 1997 amending representative prices and additional duties for the import of certain products in the sugar sector COMMISSION REGULATION (EC) No 618/97 of 8 April 1997 establishing the standard import values for determining the entry price of certain fruit and vegetables COMMISSION REGULATION (EC) No 617/97 of 8 April 1997 suspending the preferential customs duties and re-establishing the Common Customs Tariff duty on imports of uniflorous (standard) carnations originating in Israel COMMISSION REGULATION (EC) No 616/97 of 8 April 1997 amending Regulation (EEC) No 3886/92 laying down detailed rules for the application of premium schemes in the beef and veal sector COMMISSION REGULATION (EC) No 615/97 of 8 April 1997 authorizing on a temporary basis the conclusion of direct contracts between producers who are members of producer organizations and processing undertakings in the fruit and vegetable sector under Council Regulation (EC) No 2200/96 COMMISSION REGULATION (EC) No 614/97 of 8 April 1997 adjusting the total quantities set in Article 3 of Council Regulation (EEC) No 3950/92 establishing an additional levy in the milk and milk products sector COMMISSION REGULATION (EC) No 613/97 of 8 April 1997 laying down rules for the application of Council Regulation (EC) No 3072/95 as regards the conditions for granting compensatory payments under the aid scheme for rice producers END OF DOCUMENT.\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_pickle(\"test/data.pkl\")\n",
    "\n",
    "# test_df = data.get_docs_labels(\"test/text-test-corpus-STRIPPED\")\n",
    "# test_df.to_pickle(\"test/data.pkl\")\n",
    "\n",
    "test_docs = test_df[\"doc\"].values\n",
    "\n",
    "print(test_docs.shape)\n",
    "print(test_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto stocks end higher after volatile session . CHANGE CHANGE TSE 5900.37 +50.15 HI 5900.37 LO 5840.29 DJI 6611.05 +27.57 GOLD ( LONDON ) US$ 350.00 +1.90 FTSE100 4248.10 -64.80 GOLD ( NY - COMEX ) US$ 354.80 +0.70 NIKKEI 17869.59 -133.81 LME CASH NICKEL US$ 7659 +99.0 CANDLR 1.3883 LME CASH ALUM US$ 1602.0 -4.0 CAN 30-YR 107.41 -0.15 BRENT CRUDE US$ 19.09 -0.27 --------------------MARKET COMMENT---------------------------- * Toronto stocks ended higher on Tuesday , buoyed by strength in golds and banking * Computer problems due to heavy trading in Bre - X Minerals hampered session * 84 million shares traded Toronto 's key stock index ended higher on Tuesday as the saga of Bre - X Minerals Ltd and its Indonesian gold find continued to dominate Canada 's biggest stock market . The TSE 300 Index climbed 50.15 points to close at 5900.37 in heavy turnover of 84.07 million shares worth C$ 1.4 billion . But the overall market was mixed with declining issues narrowly outpacing advances 476 to 464 . 298 issues were flat . Frantic trading in Bre - X collapsed the TSE 's computer trading system earlier in the day , forcing the exchange to halt trading in the stock before the market closed . Shares in the Calgary - based gold prospector were halted for a statement by the company this morning . When it resumed , a whopping 7.7 million shares changed hands in the first 22 minutes of trading before the system crashed . Bre - X closed up 1.35 at 3.85 . It was the first time Bre - X traded since investors lopped nearly C$ 3 billion off its stock market value last Thursday . TSE officials said the trading problems were due to old technology which will be replaced . On the Montreal Exchange , Bre - X closed up 0.81 at 3.50 on 9.8 million shares . Analysts predicted more volatility for Bre - X shares this week . \" The question of what Bre - X will release over the next few days will be important to the market , \" said Josef Schachter , of Schachter Asset Management Inc. The gold sector rose nearly 136 points , leading 12 of 14 sub - indices higher . Other strong groups included financial services , consumer products , energy and transportation . The TSE posted minor losses in forestry and real estate . --- HOT STOCKS --- * Among bank shares , Bank of Nova Scotia rose 0.65 to 51.50 on 2.1 million shares , while Canadian Imperial Bank of Commerce added 0.50 to 31.80 on 2.1 million shares . ( ( Reuters Toronto Bureau ( 416 ) 941 - 8100 ) )\n"
     ]
    }
   ],
   "source": [
    "path_to_preprocessed_train_docs = f\"train/preprocessed_docs_{version}.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(path_to_preprocessed_train_docs, \"rb\") as f:\n",
    "        preprocessed_train_docs = pickle.load(f)\n",
    "except:\n",
    "    preprocessed_train_docs = preprocessing.preprocess_corpus(train_docs)\n",
    "    with open(path_to_preprocessed_train_docs, \"wb\") as f:\n",
    "        pickle.dump(preprocessed_train_docs, f)\n",
    "\n",
    "print(preprocessed_train_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OFFICIAL JOURNAL CONTENTS - OJ L 94 OF APRIL 9 , 1997 . * ( Note - contents are displayed in reverse order to that in the printed Journal ) * COMMISSION DECISION of 14 March 1997 amending the list of declining industrial areas concerned by Objective 2 as defined by Council Regulation ( EEC ) No 2052/88 ( 97/237 / EC ) COMMISSION DECISION of 13 March 1997 authorizing the grant by Finland of national aid in the reindeer sector ( Only the Finnish and Swedish texts are authentic ) ( 97/236 / EC ) COMMISSION DECISION of 6 March 1997 approving a modification to the single programming document for Community structural measures for improving the processing and marketing conditions for agricultural products in the Grand Duchy of Luxembourg , in respect of Objective 5 ( a ) , covering the period between 1994 and 1999 ( Only the French text is authentic ) ( 97/235 / EC ) COMMISSION DECISION of 3 March 1997 amending Decision 96/233 / EC establishing the list of approved fish farms in Denmark ( Text with EEA relevance ) ( 97/234 / EC ) COMMISSION REGULATION ( EC ) No 619/97 of 8 April 1997 amending representative prices and additional duties for the import of certain products in the sugar sector COMMISSION REGULATION ( EC ) No 618/97 of 8 April 1997 establishing the standard import values for determining the entry price of certain fruit and vegetables COMMISSION REGULATION ( EC ) No 617/97 of 8 April 1997 suspending the preferential customs duties and re - establishing the Common Customs Tariff duty on imports of uniflorous ( standard ) carnations originating in Israel COMMISSION REGULATION ( EC ) No 616/97 of 8 April 1997 amending Regulation ( EEC ) No 3886/92 laying down detailed rules for the application of premium schemes in the beef and veal sector COMMISSION REGULATION ( EC ) No 615/97 of 8 April 1997 authorizing on a temporary basis the conclusion of direct contracts between producers who are members of producer organizations and processing undertakings in the fruit and vegetable sector under Council Regulation ( EC ) No 2200/96 COMMISSION REGULATION ( EC ) No 614/97 of 8 April 1997 adjusting the total quantities set in Article 3 of Council Regulation ( EEC ) No 3950/92 establishing an additional levy in the milk and milk products sector COMMISSION REGULATION ( EC ) No 613/97 of 8 April 1997 laying down rules for the application of Council Regulation ( EC ) No 3072/95 as regards the conditions for granting compensatory payments under the aid scheme for rice producers END OF DOCUMENT .\n"
     ]
    }
   ],
   "source": [
    "path_to_preprocessed_test_docs = f\"test/preprocessed_docs_{version}.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(path_to_preprocessed_test_docs, \"rb\") as f:\n",
    "        preprocessed_test_docs = pickle.load(f)\n",
    "except:\n",
    "    preprocessed_test_docs = preprocessing.preprocess_corpus(test_docs)\n",
    "    with open(path_to_preprocessed_test_docs, \"wb\") as f:\n",
    "        pickle.dump(preprocessed_test_docs, f)\n",
    "\n",
    "print(preprocessed_test_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing the documents as token index sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = preprocessed_train_docs + preprocessed_test_docs\n",
    "n_vocabulary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762712\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=n_vocabulary, filters=\"\", lower=False)\n",
    "tokenizer.fit_on_texts(docs)\n",
    "word_idx = tokenizer.word_index\n",
    "if n_vocabulary is None:\n",
    "    n_vocabulary = len(word_idx) + 1 # use index 0 for padding\n",
    "\n",
    "print(n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "(332915, 1536)\n",
      "[  2095    215    136    146     52   2659    588      2   3697   3697\n",
      "   9427 235954 398775   8385 235954   8889 398776  29489 180421 287072\n",
      "   4768     14   2997     13    582  22120 130920  55161 398777 398778\n",
      "   4768     14   3289      7   3307     13    582 235955  66216  14981\n",
      " 398779 202329   1638   5552  16603    582 398780 398781  23709 163462\n",
      "   1638   5552  23013    582 398782  14921   1753  11613 123030  33682\n",
      "  15599  12721    582  41671  47418  31933  36245     38   2095    215\n",
      "    225    146     12     84      3   5798     23   1048      6  11356\n",
      "      9    819     38   3286    701    167      4    891    179      6\n",
      "   3606      7   1957   6700   9271    588     38   4184     31     75]\n"
     ]
    }
   ],
   "source": [
    "n_sequence = 1536\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "if n_sequence is None:\n",
    "    n_sequence = max([len(s) for s in sequences])\n",
    "sequence_lengths = [min(len(s), n_sequence) for s in sequences]\n",
    "sequences = pad_sequences(sequences,\n",
    "                          maxlen=n_sequence,\n",
    "                          padding=\"post\",\n",
    "                          truncating=\"post\")\n",
    "\n",
    "print(n_sequence)\n",
    "print(sequences.shape)\n",
    "print(sequences[2][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_lengths = [len(doc.split()) for doc in docs]\n",
    "\n",
    "sns.distplot(doc_lengths,\n",
    "             bins=np.arange(0, 2500, 25),\n",
    "             kde=False,\n",
    "             label=\"documents\")\n",
    "sns.distplot(sequence_lengths,\n",
    "             bins=np.arange(0, 2500, 25),\n",
    "             kde=False,\n",
    "             label=\"sequences\")\n",
    "total_word_coverage = np.round(np.sum(sequence_lengths) / np.sum(doc_lengths), 3)\n",
    "plt.title(f\"n_vocabulary={n_vocabulary}, n_sequence={n_sequence},\\n\"\n",
    "          f\"total_word_coverage={total_word_coverage}\")\n",
    "plt.xlim(0, 1550)\n",
    "plt.xlabel(\"length\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_embedding = 300 # 300 required by pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762711\n",
      "['the', '.', ',', 'to', 'of', 'in', '-', 'a', 'and', '\"', 'said', 'on', ')', '(', \"'s\", 'for', 'The', 'at', 'was', '$', 'that', 'is', 'by', 'with', 'from', 'percent', 'it', 'be', '/', 'as', 'million', 'its', 'will', 'has', '--', 'were', 'not', '*', 'would', 'year', 'are', 'have', 'an', 'he', 'which', ':', 'had', 'market', 'up', 'A', 'but', 'after', 'this', 'N', 'company', 'one', 'U.S.', 'been', 'billion', 'also', 'government', 'last', 'or', '1997', 'two', 'their', 'they', 'first', 'over', 'new', '1', 'his', 'more', 'June', 'shares', 'about', 'week', 'It', 'than', 'Bank', 'I', 'share', 'who', 'Tuesday', '1996', '%', 'per', 'Wednesday', 'May', 'we', 'Thursday', 'expected', 'July', 'Monday', 'We', 'some', 'Friday', 'down', 'could', 'three']\n"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "\n",
    "ft_path = f\"data/fasttext_{version}.model\"\n",
    "# ft_path = f\"train/fasttext_{version}.model\"\n",
    "\n",
    "try:\n",
    "    ft = FastText.load(ft_path)\n",
    "except:\n",
    "    ft = FastText(sentences=[doc.split() for doc in docs], size=n_embedding,\n",
    "                  min_count=1, workers=cpu_count(), seed=seed)\n",
    "    ft.save(ft_path)\n",
    "\n",
    "print(len(list(ft.wv.vocab)))\n",
    "print(ft.wv.index2entity[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762711\n",
      "['the', '.', ',', 'to', 'of', 'in', '-', 'a', 'and', '\"', 'said', 'on', ')', '(', \"'s\", 'for', 'The', 'at', 'was', '$', 'that', 'is', 'by', 'with', 'from', 'percent', 'it', 'be', '/', 'as', 'million', 'its', 'will', 'has', '--', 'were', 'not', '*', 'would', 'year', 'are', 'have', 'an', 'he', 'which', ':', 'had', 'market', 'up', 'A', 'but', 'after', 'this', 'N', 'company', 'one', 'U.S.', 'been', 'billion', 'also', 'government', 'last', 'or', '1997', 'two', 'their', 'they', 'first', 'over', 'new', '1', 'his', 'more', 'June', 'shares', 'about', 'week', 'It', 'than', 'Bank', 'I', 'share', 'who', 'Tuesday', '1996', '%', 'per', 'Wednesday', 'May', 'we', 'Thursday', 'expected', 'July', 'Monday', 'We', 'some', 'Friday', 'down', 'could', 'three']\n"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "\n",
    "w2v_path = f\"data/w2v_{version}.model\"\n",
    "# w2v_path = f\"train/w2v_{version}.model\"\n",
    "\n",
    "try:\n",
    "    w2v = Word2Vec.load(w2v_path)\n",
    "except:\n",
    "    w2v = Word2Vec(sentences=[doc.split() for doc in docs], size=n_embedding,\n",
    "                   min_count=1, workers=cpu_count(), seed=seed)\n",
    "    w2v.save(w2v_path)\n",
    "\n",
    "print(len(list(w2v.wv.vocab)))\n",
    "print(w2v.wv.index2entity[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(fname, skip_first):\n",
    "    embedding_idx = {}\n",
    "    with open(fname, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == 0 and skip_first:\n",
    "                continue\n",
    "            vals = line.rstrip().split()\n",
    "            token = \"\".join(vals[:-300])\n",
    "            embedding = np.array(vals[-300:], dtype=np.float32)\n",
    "            embedding_idx[token] = embedding\n",
    "    return embedding_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'and', 'to', 'of', 'a', 'in', 'is', 'for', 'that', 'I', 'it', 'on', 'with', ')', ':', '\"', '(', 'The', 'you', 'was', 'are', 'or', 'this', 'as', 'have', 'at', 'from', '!', \"'s\", 'but', 'by', 'not', '?', 'your', 'all', '/', 'be', 'we', 'my', 'one', '-', 'will', 'they', 'so', 'which', '”', '“', 'has', '...', 'just', 'he', 'their', 'can', 'about', 'his', 'our', ';', 'when', 'more', 'had', 'do', 'some', 'time', 'like', 'also', 'there', 'them', 'get', 'what', 'out', \"'\", 'me', 'her', 'an', 'were', 'This', 'It', 'up', 'would', 'if', 'who', 'new', 'only', 'A', '–', 'people', 'any', 'We', 'make', 'other', 'In', 'then', 'its', 'use', 'said', 'now', 'no', 'first']\n"
     ]
    }
   ],
   "source": [
    "ft_pretrained = load_embeddings(\"data/crawl-300d-2M.vec\", skip_first=True)\n",
    "\n",
    "token_iter = iter(ft_pretrained)\n",
    "print([next(token_iter) for i in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', 'and', 'to', 'of', 'a', 'in', '\"', ':', 'is', 'for', 'I', ')', '(', 'that', '-', 'on', 'you', 'with', \"'s\", 'it', 'The', 'are', 'by', 'at', 'be', 'this', 'as', 'from', 'was', 'have', 'or', '...', 'your', 'not', '!', '?', 'will', 'an', \"n't\", 'can', 'but', 'all', 'my', 'has', '|', 'do', 'we', 'they', 'more', 'one', 'about', 'he', ';', \"'\", 'out', '$', 'their', 'so', 'his', 'up', 'It', '&', 'like', '/', '1', 'which', 'if', 'would', 'our', '[', ']', 'me', 'who', 'just', 'This', 'time', 'what', 'A', '2', 'had', 'when', 'there', 'been', 'some', 'get', 'were', 'other', 'also', 'In', 'her', 'them', 'You', 'new', 'We', 'no', 'any', '>', 'people']\n"
     ]
    }
   ],
   "source": [
    "if version == \"tokenized_cased\":\n",
    "    glove_pretrained = load_embeddings(\"data/glove.840B.300d.txt\", skip_first=False)\n",
    "else:\n",
    "    glove_pretrained = load_embeddings(\"data/glove.42B.300d.txt\", skip_first=False)\n",
    "    \n",
    "token_iter = iter(glove_pretrained)\n",
    "print([next(token_iter) for i in range(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(embedding_model):\n",
    "    embedding_matrix = np.zeros((n_vocabulary, n_embedding))\n",
    "    unknown_token_count = 0\n",
    "    for token, i in word_idx.items():\n",
    "        if i >= n_vocabulary:\n",
    "            continue\n",
    "        if token in embedding_model:\n",
    "            embedding_matrix[i] = embedding_model[token]\n",
    "        else:\n",
    "            unknown_token_count += 1\n",
    "\n",
    "    print(unknown_token_count)\n",
    "    print(embedding_matrix.shape)\n",
    "    print(embedding_matrix[1][:20])\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(762712, 300)\n",
      "[ 2.60082126  4.1615696   3.10287261  4.46255398  0.49275851 -3.08360028\n",
      "  5.75870943  3.70959353  0.92910308 -0.94961739  0.0122972   5.87578487\n",
      "  0.45075765 -3.08726764 -1.17578566  2.33939171 -0.92189437 -0.80971539\n",
      "  1.7391876  -2.2687695 ]\n"
     ]
    }
   ],
   "source": [
    "ft_embedding_matrix = create_embedding_matrix(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(762712, 300)\n",
      "[-0.63477027 -0.77177155 -0.5531857   1.17861974  0.95079625 -0.01210889\n",
      " -0.86642587 -0.2856015  -1.40522826  0.75717586 -1.6686604   2.61663437\n",
      " -2.13727951 -0.03661137 -1.20471513 -1.72690833  0.86653012  0.21646138\n",
      "  0.04978285 -0.05401069]\n"
     ]
    }
   ],
   "source": [
    "w2v_embedding_matrix = create_embedding_matrix(w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499966\n",
      "(762712, 300)\n",
      "[ 0.0231      0.017       0.0157     -0.0773      0.1088      0.0031\n",
      " -0.1487     -0.26719999 -0.0357     -0.0487      0.0807      0.1532\n",
      " -0.0739     -0.0291     -0.0445     -0.0014      0.1014      0.0186\n",
      " -0.0253      0.02      ]\n"
     ]
    }
   ],
   "source": [
    "ft_pretrained_embedding_matrix = create_embedding_matrix(ft_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463359\n",
      "(762712, 300)\n",
      "[ 0.27204001 -0.06203    -0.1884      0.023225   -0.018158    0.0067192\n",
      " -0.13877     0.17708001  0.17709     2.58820009 -0.35179001 -0.17312001\n",
      "  0.43285    -0.10708     0.15006    -0.19982    -0.19092999  1.18710005\n",
      " -0.16207001 -0.23537999]\n"
     ]
    }
   ],
   "source": [
    "glove_pretrained_embedding_matrix = create_embedding_matrix(glove_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = None\n",
    "x_train, y_train = shuffle(sequences[:n_train],\n",
    "                           train_labels,\n",
    "                           random_state=seed,\n",
    "                           n_samples=n_samples)\n",
    "x_test = sequences[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_evaluate(model_initializer, batch_size=256, model_params={}):\n",
    "    model = model_initializer(n_vocabulary, n_embedding, n_sequence, n_labels,\n",
    "                      **model_params)\n",
    "    model.summary()\n",
    "    plot_model(model, to_file=f\"model_plots/{model_initializer.__name__}_{version}.png\",\n",
    "               show_shapes=True)\n",
    "\n",
    "    cv_scores = []\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, random_state=seed)\n",
    "    for train, val in mskf.split(x_train, y_train):\n",
    "        model = model_initializer(n_vocabulary, n_embedding, n_sequence, n_labels,\n",
    "                                  **model_params)\n",
    "        es = EarlyStopping(patience=10, verbose=1, restore_best_weights=True)\n",
    "        history = model.fit(x_train[train],\n",
    "                            y_train[train],\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=100,\n",
    "                            verbose=1,\n",
    "                            validation_data=(x_train[val], y_train[val]),\n",
    "                            callbacks=[es])\n",
    "\n",
    "        y_val_pred_prob = model.predict(x_train[val], batch_size=batch_size, verbose=1)\n",
    "        y_val_pred = np.round(y_val_pred_prob)\n",
    "\n",
    "        scores = {}\n",
    "        scores[\"accuracy\"] = accuracy_score(y_train[val], y_val_pred)\n",
    "        scores[\"F1 (macro)\"] = f1_score(y_train[val], y_val_pred, average=\"macro\")\n",
    "        scores[\"F1 (micro)\"] = f1_score(y_train[val], y_val_pred, average=\"micro\")\n",
    "        scores[\"LRAP\"] = label_ranking_average_precision_score(y_train[val],\n",
    "                                                               y_val_pred_prob)\n",
    "        scores[\"NDCG\"] = ndcg_score(y_train[val], y_val_pred_prob)\n",
    "        scores[\"timestamp\"] = round(datetime.timestamp(datetime.now()))\n",
    "        cv_scores.append(scores)\n",
    "        print(scores)\n",
    "\n",
    "        y_test_pred_prob = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "        y_test_pred = np.round(y_test_pred_prob)\n",
    "        \n",
    "        np.savetxt(f\"test_results/{model_initializer.__name__}_{version}_\" +\n",
    "                   f\"{scores['timestamp']}_\" +\n",
    "                   f\"{np.round(scores['F1 (micro)'], 6)}.txt\", y_test_pred, fmt=\"%d\")\n",
    "\n",
    "    cv_scores_df = pd.DataFrame(cv_scores)\n",
    "    display(cv_scores_df)\n",
    "    print(cv_scores_df.drop(\"timestamp\", axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 1536, 300)         228813600 \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 512)               1140736   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 230,018,974\n",
      "Trainable params: 1,205,374\n",
      "Non-trainable params: 228,813,600\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "937/937 [==============================] - 165s 177ms/step - loss: 0.0133 - val_loss: 0.0077\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 156s 167ms/step - loss: 0.0067 - val_loss: 0.0060\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 157s 167ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 157s 168ms/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 157s 167ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 157s 167ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 157s 167ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 157s 168ms/step - loss: 0.0042 - val_loss: 0.0044\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 157s 167ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 157s 167ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 157s 167ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 156s 167ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 155s 166ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 156s 166ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 156s 167ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "851/937 [==========================>...] - ETA: 12s - loss: 0.0034"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "cross_evaluate(models.bi_lstm_1, model_params={\n",
    "    \"filters_1\": 400, \"filters_2\": 500, \"loss\": \"mean_squared_error\",\n",
    "    \"embedding_matrix\": ft_pretrained_embedding_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1536, 300)         228813600 \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1535, 400)         240400    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1535, 400)         1600      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 1535, 400)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 767, 400)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 767, 400)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 766, 500)          400500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 766, 500)          2000      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 766, 500)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 383, 500)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 383, 500)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 512)               1550336   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 231,073,074\n",
      "Trainable params: 2,257,674\n",
      "Non-trainable params: 228,815,400\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1\n",
      "937/937 [==============================] - 121s 129ms/step - loss: 0.0127 - val_loss: 0.0068\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0060 - val_loss: 0.0052\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 118s 125ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 121s 129ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 116s 123ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 117s 124ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0030Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 119s 128ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 00022: early stopping\n",
      "235/235 [==============================] - 9s 40ms/step\n",
      "{'accuracy': 0.6958450786449385, 'F1 (macro)': 0.5815655797116668, 'F1 (micro)': 0.893319812262408, 'LRAP': 0.9570028415750389, 'NDCG': 0.9745507346337073, 'timestamp': 1590751403}\n",
      "130/130 [==============================] - 5s 41ms/step\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0121 - val_loss: 0.0068\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0059 - val_loss: 0.0052\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 117s 124ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 117s 124ms/step - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0033 - val_loss: 0.0043\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 117s 124ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 117s 124ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0030Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 120s 128ms/step - loss: 0.0030 - val_loss: 0.0043\n",
      "Epoch 00023: early stopping\n",
      "235/235 [==============================] - 9s 40ms/step\n",
      "{'accuracy': 0.695761679982653, 'F1 (macro)': 0.5816796773163075, 'F1 (micro)': 0.8933951764499138, 'LRAP': 0.9567783374197022, 'NDCG': 0.9736582839547232, 'timestamp': 1590754175}\n",
      "130/130 [==============================] - 5s 40ms/step\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1\n",
      "937/937 [==============================] - 127s 136ms/step - loss: 0.0126 - val_loss: 0.0067\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0060 - val_loss: 0.0052\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 118s 125ms/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 118s 125ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 118s 125ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - 117s 124ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 24/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 25/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 26/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 27/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 28/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 29/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0028Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 120s 128ms/step - loss: 0.0028 - val_loss: 0.0043\n",
      "Epoch 00029: early stopping\n",
      "235/235 [==============================] - 9s 40ms/step\n",
      "{'accuracy': 0.7010556861960274, 'F1 (macro)': 0.5991152346587585, 'F1 (micro)': 0.8957544697455011, 'LRAP': 0.9574185614015702, 'NDCG': 0.9773251411317034, 'timestamp': 1590757639}\n",
      "130/130 [==============================] - 5s 39ms/step\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1\n",
      "937/937 [==============================] - 121s 129ms/step - loss: 0.0124 - val_loss: 0.0069\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0060 - val_loss: 0.0053\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0051 - val_loss: 0.0049\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 118s 125ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0034 - val_loss: 0.0043\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 117s 124ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0031 - val_loss: 0.0043\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 24/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 25/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 26/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0029Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 120s 128ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 00026: early stopping\n",
      "235/235 [==============================] - 9s 40ms/step\n",
      "{'accuracy': 0.697012659916935, 'F1 (macro)': 0.5963220271983352, 'F1 (micro)': 0.8939271983986613, 'LRAP': 0.9570173768729222, 'NDCG': 0.9740214961568645, 'timestamp': 1590760751}\n",
      "130/130 [==============================] - 5s 40ms/step\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = nccl, num_packs = 1\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0122 - val_loss: 0.0067\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0059 - val_loss: 0.0052\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 118s 126ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 117s 124ms/step - loss: 0.0030 - val_loss: 0.0043\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - 117s 125ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 24/100\n",
      "937/937 [==============================] - 116s 124ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 25/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0029Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 119s 127ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 00025: early stopping\n",
      "235/235 [==============================] - 9s 40ms/step\n",
      "{'accuracy': 0.6988974696845862, 'F1 (macro)': 0.5939369658206258, 'F1 (micro)': 0.8941441680369171, 'LRAP': 0.9570362638403476, 'NDCG': 0.9755676631618307, 'timestamp': 1590763757}\n",
      "130/130 [==============================] - 5s 40ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1 (macro)</th>\n",
       "      <th>F1 (micro)</th>\n",
       "      <th>LRAP</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.695845</td>\n",
       "      <td>0.581566</td>\n",
       "      <td>0.893320</td>\n",
       "      <td>0.957003</td>\n",
       "      <td>0.974551</td>\n",
       "      <td>1590751403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.695762</td>\n",
       "      <td>0.581680</td>\n",
       "      <td>0.893395</td>\n",
       "      <td>0.956778</td>\n",
       "      <td>0.973658</td>\n",
       "      <td>1590754175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.701056</td>\n",
       "      <td>0.599115</td>\n",
       "      <td>0.895754</td>\n",
       "      <td>0.957419</td>\n",
       "      <td>0.977325</td>\n",
       "      <td>1590757639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.697013</td>\n",
       "      <td>0.596322</td>\n",
       "      <td>0.893927</td>\n",
       "      <td>0.957017</td>\n",
       "      <td>0.974021</td>\n",
       "      <td>1590760751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.698897</td>\n",
       "      <td>0.593937</td>\n",
       "      <td>0.894144</td>\n",
       "      <td>0.957036</td>\n",
       "      <td>0.975568</td>\n",
       "      <td>1590763757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  F1 (macro)  F1 (micro)      LRAP      NDCG   timestamp\n",
       "0  0.695845    0.581566    0.893320  0.957003  0.974551  1590751403\n",
       "1  0.695762    0.581680    0.893395  0.956778  0.973658  1590754175\n",
       "2  0.701056    0.599115    0.895754  0.957419  0.977325  1590757639\n",
       "3  0.697013    0.596322    0.893927  0.957017  0.974021  1590760751\n",
       "4  0.698897    0.593937    0.894144  0.957036  0.975568  1590763757"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy      0.697715\n",
      "F1 (macro)    0.590524\n",
      "F1 (micro)    0.894108\n",
      "LRAP          0.957051\n",
      "NDCG          0.975025\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "cross_evaluate(models.cnn_bi_lstm_1, model_params={\n",
    "    \"filters_1\": 400, \"filters_2\": 500, \"loss\": \"mean_squared_error\",\n",
    "    \"embedding_matrix\": ft_pretrained_embedding_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed()\n",
    "cross_evaluate(models.cnn_bi_lstm_1, model_params={\n",
    "    \"filters_1\": 400, \"filters_2\": 500, \"loss\": \"binary_crossentropy\",\n",
    "    \"embedding_matrix\": ft_pretrained_embedding_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1536, 300)         228813600 \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1535, 400)         240400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1535, 400)         1600      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 1535, 400)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 767, 400)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 767, 400)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 766, 500)          400500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 766, 500)          2000      \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 766, 500)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 383, 500)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 383, 500)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 383, 512)          1550336   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 256)               656384    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 126)               32382     \n",
      "=================================================================\n",
      "Total params: 231,697,202\n",
      "Trainable params: 2,881,802\n",
      "Non-trainable params: 228,815,400\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n",
      "937/937 [==============================] - 186s 198ms/step - loss: 0.0159 - val_loss: 0.0094\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0075 - val_loss: 0.0062\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 183s 196ms/step - loss: 0.0057 - val_loss: 0.0053\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 179s 192ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0044 - val_loss: 0.0046\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0042 - val_loss: 0.0045\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 179s 192ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 179s 192ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 179s 192ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 179s 192ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 179s 192ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 24/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 25/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 26/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 27/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 28/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 29/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 30/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 31/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0028Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 182s 194ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 00031: early stopping\n",
      "235/235 [==============================] - 16s 66ms/step\n",
      "{'accuracy': 0.7027171284172602, 'F1 (macro)': 0.5869305836298186, 'F1 (micro)': 0.8942728463282972, 'LRAP': 0.9572202049554496, 'NDCG': 0.974730795923536, 'timestamp': 1590771319}\n",
      "130/130 [==============================] - 9s 66ms/step\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n",
      "937/937 [==============================] - 185s 198ms/step - loss: 0.0153 - val_loss: 0.0089\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0073 - val_loss: 0.0061\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0057 - val_loss: 0.0053\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0044 - val_loss: 0.0046\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0042 - val_loss: 0.0045\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 180s 193ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 178s 189ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 178s 189ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 24/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 25/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 26/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 27/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0029Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 181s 193ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 00027: early stopping\n",
      "235/235 [==============================] - 16s 67ms/step\n",
      "{'accuracy': 0.7005487631978383, 'F1 (macro)': 0.5777331593475916, 'F1 (micro)': 0.8939218365819287, 'LRAP': 0.9566382070663109, 'NDCG': 0.973596694776429, 'timestamp': 1590776235}\n",
      "130/130 [==============================] - 9s 66ms/step\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n",
      "937/937 [==============================] - 184s 197ms/step - loss: 0.0158 - val_loss: 0.0095\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0076 - val_loss: 0.0061\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0058 - val_loss: 0.0052\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0051 - val_loss: 0.0049\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0043 - val_loss: 0.0044\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 179s 192ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 179s 192ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0033 - val_loss: 0.0041\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0032 - val_loss: 0.0041\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0031 - val_loss: 0.0041\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - 185s 197ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 24/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 25/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 26/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 27/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 28/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 29/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 30/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 31/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0028Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 182s 194ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 00031: early stopping\n",
      "235/235 [==============================] - 16s 66ms/step\n",
      "{'accuracy': 0.7058588082253465, 'F1 (macro)': 0.5878249540229905, 'F1 (micro)': 0.8958778464067014, 'LRAP': 0.9569023390480772, 'NDCG': 0.9770946109118293, 'timestamp': 1590781885}\n",
      "130/130 [==============================] - 9s 66ms/step\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 22 all-reduces with algorithm = nccl, num_packs = 1\n",
      "937/937 [==============================] - 184s 197ms/step - loss: 0.0158 - val_loss: 0.0092\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 179s 192ms/step - loss: 0.0075 - val_loss: 0.0061\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0057 - val_loss: 0.0053\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0050 - val_loss: 0.0049\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0046 - val_loss: 0.0047\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0042 - val_loss: 0.0044\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0038 - val_loss: 0.0042\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 177s 189ms/step - loss: 0.0035 - val_loss: 0.0043\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937/937 [==============================] - 178s 189ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 178s 189ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 177s 189ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - 177s 189ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 24/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 25/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 26/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0029Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 00026: early stopping\n",
      "235/235 [==============================] - 16s 66ms/step\n",
      "{'accuracy': 0.701966540456691, 'F1 (macro)': 0.5727928691228157, 'F1 (micro)': 0.8942736306652401, 'LRAP': 0.9568992184005976, 'NDCG': 0.9741928728206998, 'timestamp': 1590786613}\n",
      "130/130 [==============================] - 9s 66ms/step\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "937/937 [==============================] - 185s 198ms/step - loss: 0.0164 - val_loss: 0.0099\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0078 - val_loss: 0.0063\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 181s 193ms/step - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0052 - val_loss: 0.0050\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0045 - val_loss: 0.0046\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0039 - val_loss: 0.0043\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0038 - val_loss: 0.0043\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0035 - val_loss: 0.0042\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 180s 193ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0034 - val_loss: 0.0042\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0033 - val_loss: 0.0042\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0032 - val_loss: 0.0042\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0031 - val_loss: 0.0042\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 24/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 25/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0030 - val_loss: 0.0042\n",
      "Epoch 26/100\n",
      "937/937 [==============================] - 179s 191ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 27/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0029 - val_loss: 0.0042\n",
      "Epoch 28/100\n",
      "937/937 [==============================] - 183s 195ms/step - loss: 0.0029 - val_loss: 0.0043\n",
      "Epoch 29/100\n",
      "937/937 [==============================] - 180s 192ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 30/100\n",
      "937/937 [==============================] - 178s 190ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 31/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0028Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 182s 194ms/step - loss: 0.0028 - val_loss: 0.0042\n",
      "Epoch 00031: early stopping\n",
      "235/235 [==============================] - 16s 66ms/step\n",
      "{'accuracy': 0.7071038980534752, 'F1 (macro)': 0.5851063938850445, 'F1 (micro)': 0.89501315835082, 'LRAP': 0.9564710304065843, 'NDCG': 0.9751378266721157, 'timestamp': 1590792276}\n",
      "130/130 [==============================] - 9s 66ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1 (macro)</th>\n",
       "      <th>F1 (micro)</th>\n",
       "      <th>LRAP</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.702717</td>\n",
       "      <td>0.586931</td>\n",
       "      <td>0.894273</td>\n",
       "      <td>0.957220</td>\n",
       "      <td>0.974731</td>\n",
       "      <td>1590771319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.700549</td>\n",
       "      <td>0.577733</td>\n",
       "      <td>0.893922</td>\n",
       "      <td>0.956638</td>\n",
       "      <td>0.973597</td>\n",
       "      <td>1590776235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.705859</td>\n",
       "      <td>0.587825</td>\n",
       "      <td>0.895878</td>\n",
       "      <td>0.956902</td>\n",
       "      <td>0.977095</td>\n",
       "      <td>1590781885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.701967</td>\n",
       "      <td>0.572793</td>\n",
       "      <td>0.894274</td>\n",
       "      <td>0.956899</td>\n",
       "      <td>0.974193</td>\n",
       "      <td>1590786613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.707104</td>\n",
       "      <td>0.585106</td>\n",
       "      <td>0.895013</td>\n",
       "      <td>0.956471</td>\n",
       "      <td>0.975138</td>\n",
       "      <td>1590792276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  F1 (macro)  F1 (micro)      LRAP      NDCG   timestamp\n",
       "0  0.702717    0.586931    0.894273  0.957220  0.974731  1590771319\n",
       "1  0.700549    0.577733    0.893922  0.956638  0.973597  1590776235\n",
       "2  0.705859    0.587825    0.895878  0.956902  0.977095  1590781885\n",
       "3  0.701967    0.572793    0.894274  0.956899  0.974193  1590786613\n",
       "4  0.707104    0.585106    0.895013  0.956471  0.975138  1590792276"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy      0.703639\n",
      "F1 (macro)    0.582078\n",
      "F1 (micro)    0.894672\n",
      "LRAP          0.956826\n",
      "NDCG          0.974951\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "cross_evaluate(models.cnn_bi_lstm_2, model_params={\n",
    "    \"filters_1\": 400, \"filters_2\": 500, \"loss\": \"mean_squared_error\",\n",
    "    \"embedding_matrix\": ft_pretrained_embedding_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed()\n",
    "cross_evaluate(models.bi_lstm_1, model_params={\n",
    "    \"filters_1\": 400, \"filters_2\": 500, \"loss\": \"mean_squared_error\",\n",
    "    \"embedding_matrix\": glove_pretrained_embedding_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed()\n",
    "cross_evaluate(models.cnn_bi_lstm_1, model_params={\n",
    "    \"filters_1\": 400, \"filters_2\": 500, \"loss\": \"mean_squared_error\",\n",
    "    \"embedding_matrix\": glove_pretrained_embedding_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed()\n",
    "cross_evaluate(models.cnn_bi_lstm_2, model_params={\n",
    "    \"filters_1\": 400, \"filters_2\": 500, \"loss\": \"mean_squared_error\",\n",
    "    \"embedding_matrix\": glove_pretrained_embedding_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed()\n",
    "cross_evaluate(models.cnn_bi_lstm_1, model_params={\n",
    "    \"filters_1\": 400, \"filters_2\": 500, \"loss\": \"mean_squared_error\",\n",
    "    \"embedding_matrix\": w2v_embedding_matrix})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seed()\n",
    "cross_evaluate(models.split_cnn_bi_lstm_2, model_params={\n",
    "    \"filters_1\": 400, \"filters_2\": 500, \"loss\": \"mean_squared_error\",\n",
    "    \"embedding_matrix_1\": ft_pretrained_embedding_matrix,\n",
    "    \"embedding_matrix_2\": glove_pretrained_embedding_matrix})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
