{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import random as rn\n",
    "import warnings\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import accuracy_score, f1_score, hamming_loss, \\\n",
    "        average_precision_score, ndcg_score, \\\n",
    "        label_ranking_average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.distribute import MirroredStrategy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant, GlorotUniform\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, \\\n",
    "        GlobalMaxPooling1D, SpatialDropout1D, LSTM, GRU, Flatten, MaxPooling1D, \\\n",
    "        BatchNormalization, ReLU, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import data\n",
    "import preprocessing\n",
    "\n",
    "seed = 42\n",
    "sns.set()\n",
    "\n",
    "def reset_seed():\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299773,)\n",
      "(299773, 126)\n",
      "Toronto stocks end higher after volatile session. CHANGE\t\t\t\t    CHANGE TSE\t  5900.37    +50.15   HI 5900.37\t    LO  5840.29 DJI\t  6611.05    +27.57   GOLD (LONDON)   US$350.00 +1.90 FTSE100    4248.10    -64.80   GOLD (NY-COMEX) US$354.80 +0.70 NIKKEI    17869.59   -133.81   LME CASH NICKEL US$7659   +99.0 CANDLR\t1.3883\t\t LME CASH ALUM   US$1602.0  -4.0 CAN 30-YR   107.41     -0.15   BRENT CRUDE     US$19.09  -0.27 --------------------MARKET COMMENT---------------------------- * Toronto stocks ended higher on Tuesday, buoyed by strength in golds and banking * Computer problems due to heavy trading in Bre-X Minerals hampered session * 84 million shares traded Toronto's key stock index ended higher on Tuesday as the saga of Bre-X Minerals Ltd and its Indonesian gold find continued to dominate Canada's biggest stock market. The TSE 300 Index climbed 50.15 points to close at 5900.37 in heavy turnover of 84.07 million shares worth C$1.4 billion. But the overall market was mixed with declining issues narrowly outpacing advances 476 to 464. 298 issues were flat. Frantic trading in Bre-X collapsed the TSE's computer trading system earlier in the day, forcing the exchange to halt trading in the stock before the market closed. Shares in the Calgary-based gold prospector were halted for a statement by the company this morning. When it resumed, a whopping 7.7 million shares changed hands in the first 22 minutes of trading before the system crashed. Bre-X closed up 1.35 at 3.85. It was the first time Bre-X traded since investors lopped nearly C$3 billion off its stock market value last Thursday. TSE officials said the trading problems were due to old technology which will be replaced. On the Montreal Exchange, Bre-X closed up 0.81 at 3.50 on 9.8 million shares. Analysts predicted more volatility for Bre-X shares this week. \"The question of what Bre-X will release over the next few days will be important to the market,\" said Josef Schachter, of Schachter Asset Management Inc. The gold sector rose nearly 136 points, leading 12 of 14 sub-indices higher. Other strong groups included financial services, consumer products, energy and transportation. The TSE posted minor losses in forestry and real estate. --- HOT STOCKS --- * Among bank shares, Bank of Nova Scotia rose 0.65 to 51.50 on 2.1 million shares, while Canadian Imperial Bank of Commerce added 0.50 to 31.80 on 2.1 million shares. ((Reuters Toronto Bureau (416) 941-8100))\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# data.extract_data(extraction_dir=\"train\",\n",
    "#                   data_dir=\"data\",\n",
    "#                   data_zip_name=\"reuters-training-corpus.zip\")\n",
    "\n",
    "train_df = pd.read_pickle(\"train/data.pkl\")\n",
    "\n",
    "# train_df = data.get_docs_labels(\"train/REUTERS_CORPUS_2\")\n",
    "# train_df.to_pickle(\"train/data.pkl\")\n",
    "\n",
    "train_docs = train_df[\"doc\"].values\n",
    "train_labels = np.array(train_df[\"labels\"].tolist())\n",
    "n_labels = len(data.CODEMAP)\n",
    "\n",
    "print(train_docs.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_docs[2])\n",
    "print(train_labels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto stocks end higher after volatile session . CHANGE CHANGE TSE 5900.37 +50.15 HI 5900.37 LO 5840.29 DJI 6611.05 +27.57 GOLD ( LONDON ) US$ 350.00 +1.90 FTSE100 4248.10 -64.80 GOLD ( NY - COMEX ) US$ 354.80 +0.70 NIKKEI 17869.59 -133.81 LME CASH NICKEL US$ 7659 +99.0 CANDLR 1.3883 LME CASH ALUM US$ 1602.0 -4.0 CAN 30-YR 107.41 -0.15 BRENT CRUDE US$ 19.09 -0.27 --------------------MARKET COMMENT---------------------------- * Toronto stocks ended higher on Tuesday , buoyed by strength in golds and banking * Computer problems due to heavy trading in Bre - X Minerals hampered session * 84 million shares traded Toronto 's key stock index ended higher on Tuesday as the saga of Bre - X Minerals Ltd and its Indonesian gold find continued to dominate Canada 's biggest stock market . The TSE 300 Index climbed 50.15 points to close at 5900.37 in heavy turnover of 84.07 million shares worth C$ 1.4 billion . But the overall market was mixed with declining issues narrowly outpacing advances 476 to 464 . 298 issues were flat . Frantic trading in Bre - X collapsed the TSE 's computer trading system earlier in the day , forcing the exchange to halt trading in the stock before the market closed . Shares in the Calgary - based gold prospector were halted for a statement by the company this morning . When it resumed , a whopping 7.7 million shares changed hands in the first 22 minutes of trading before the system crashed . Bre - X closed up 1.35 at 3.85 . It was the first time Bre - X traded since investors lopped nearly C$ 3 billion off its stock market value last Thursday . TSE officials said the trading problems were due to old technology which will be replaced . On the Montreal Exchange , Bre - X closed up 0.81 at 3.50 on 9.8 million shares . Analysts predicted more volatility for Bre - X shares this week . \" The question of what Bre - X will release over the next few days will be important to the market , \" said Josef Schachter , of Schachter Asset Management Inc. The gold sector rose nearly 136 points , leading 12 of 14 sub - indices higher . Other strong groups included financial services , consumer products , energy and transportation . The TSE posted minor losses in forestry and real estate . --- HOT STOCKS --- * Among bank shares , Bank of Nova Scotia rose 0.65 to 51.50 on 2.1 million shares , while Canadian Imperial Bank of Commerce added 0.50 to 31.80 on 2.1 million shares . ( ( Reuters Toronto Bureau ( 416 ) 941 - 8100 ) )\n"
     ]
    }
   ],
   "source": [
    "path_to_preprocessed_docs = \"train/preprocessed_docs.pkl\"\n",
    "# path_to_preprocessed_docs = \"train/preprocessed_docs_lemmatized_no_sw.pkl\"\n",
    "\n",
    "with open(path_to_preprocessed_docs, \"rb\") as f:\n",
    "    preprocessed_train_docs = pickle.load(f)\n",
    "\n",
    "# preprocessed_train_docs = preprocessing.preprocess_corpus(train_docs)\n",
    "# with open(path_to_preprocessed_docs, \"wb\") as f:\n",
    "#     pickle.dump(preprocessed_train_docs, f)\n",
    "\n",
    "print(preprocessed_train_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = preprocessed_train_docs # choose between original and preprocessed versions\n",
    "n_vocabulary = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As token index sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=n_vocabulary, filters=\"\")\n",
    "tokenizer.fit_on_texts(docs)\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "if n_vocabulary is None:\n",
    "    n_vocabulary = len(word_idx)\n",
    "\n",
    "print(n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "(299773, 512)\n",
      "[ 1916   206   140   150    54  2485   629     2   355   355  8304  5370\n",
      "  6980   319    14   205    13   628 19464   319    14  3066     8  2860\n",
      "    13   628  2705  1639   345  2206   628  1639   345 18753   628 13173\n",
      "   168  9703  1110   557   628    40  1916   206   169   150    12    89\n",
      "     3  5117    23  1053     6  9281     9   686    40  1029   723   160\n",
      "     4   805   163     6  3078     8  1808  4537  8305   629    40  3937\n",
      "    30    67   448  1916    15   424   120   138   169   150    12    89\n",
      "    28     1  8953     5  3078     8  1808  4537   214     9    32  1715\n",
      "   319  1180   626     4  6759   622    15   744   120    46     2     1\n",
      "  8304   930   138  2269   141     4   185    17     6   805   591     5\n",
      "    30    67   673  1135  2055    68     2    35     1   867    46    18\n",
      "  1149    24  3401   500  5082 17234  3869     4 18754     2 14109   500\n",
      "    38  1082     2 17628   163     6  3078     8  1808  3947     1  8304\n",
      "    15  1029   163   374   230     6     1   131     3  4316     1   180\n",
      "     4  2866   163     6     1   120   142     1    46   268     2    67\n",
      "     6     1  7136     8   249   319    38  4299    16     7   189    23\n",
      "     1    57    50   553     2   124    22  3756     3     7  5296    30\n",
      "    67  1103  1508     6     1    62   382  1465     5   163   142     1\n",
      "   374  5428     2  3078     8  1808   268    49  6170    17  8534     2\n",
      "    22    18     1    62   132  3078     8  1808   448   167   264   868\n",
      "  1135   170    68   162    32   120    46   503    60   100     2  8304\n",
      "   236    11     1   163   723    38   160     4   570   784    45    33\n",
      "    27  3382     2    12     1  4446   180     3  3078     8  1808   268\n",
      "    49  9981    17  4228    12  6235    30    67     2   186  1997    74\n",
      "  2063    16  3078     8  1808    67    50    80     2    10     1  1402\n",
      "     5   285  3078     8  1808    33   970    73     1   147   485   293\n",
      "    33    27   904     4     1    46     3    10    11 11833     3     5\n",
      "  1459   550   497     1   319   356   175   868  8249   141     3   444\n",
      "   287     5   392  3210     8  2845   150     2   101   281   920   977\n",
      "   183   351     3   564   353     3   584     9  2727     2     1  8304\n",
      "  1258  3510   638     6  5602     9   627  1832     2   671  2784   206\n",
      "   671    40   536    52    67     3    52     5  4713  6595   175  6171\n",
      "     4    12  2908    30    67     3   144   901  5492    52     5  1687\n",
      "   173  2029     4    12  2908    30    67     2    14    14   192  1916\n",
      "  1028    14  4266    13  1951     8  6694    13    13     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_sequence = 512\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "sequence_lengths = [len(s) for s in sequences]\n",
    "if n_sequence is None:\n",
    "    n_sequence = max(sequence_lengths)\n",
    "sequences = pad_sequences(sequences,\n",
    "                          maxlen=n_sequence,\n",
    "                          padding=\"post\",\n",
    "                          truncating=\"post\")\n",
    "\n",
    "print(n_sequence)\n",
    "print(sequences.shape)\n",
    "print(sequences[2])\n",
    "\n",
    "sns.distplot(sequence_lengths, bins=np.arange(0, 1000, 25), kde=False)\n",
    "plt.title(f\"Document lengths (n_vocabulary={n_vocabulary})\")\n",
    "plt.xlim(0, 1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_term_matrix = tokenizer.texts_to_matrix(docs, mode=\"tfidf\")\n",
    "\n",
    "# print(doc_term_matrix.shape)\n",
    "# print(doc_term_matrix[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_embedding = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# reset_seed()\n",
    "\n",
    "# window = 5\n",
    "# w2v_path = f\"data/w2v_{window}_{n_embedding}.model\"\n",
    "\n",
    "# try:\n",
    "#     embedding_model = Word2Vec.load(w2v_path)\n",
    "# except:\n",
    "#     embedding_model = Word2Vec(sentences=[s.split() for s in docs],\n",
    "#                                size=n_embedding, \n",
    "#                                window=window,\n",
    "#                                min_count=5,\n",
    "#                                sg=1,\n",
    "#                                workers=cpu_count(),\n",
    "#                                seed=seed)\n",
    "#     embedding_model.save(w2v_path)\n",
    "\n",
    "# print(len(list(embedding_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    return data\n",
    "\n",
    "embedding_model = load_vectors(\"data/wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 300)\n",
      "[ 8.970e-02  1.600e-02 -5.710e-02  4.050e-02 -6.960e-02 -1.237e-01\n",
      "  3.010e-02  2.480e-02 -3.030e-02  1.740e-02  6.300e-03  1.840e-02\n",
      "  2.170e-02 -2.570e-02  3.500e-02 -2.420e-02  2.900e-03  1.880e-02\n",
      " -5.700e-02  2.520e-02 -2.100e-02 -8.000e-04  3.600e-02 -7.290e-02\n",
      " -6.650e-02  9.890e-02  6.760e-02  8.520e-02 -8.900e-03  3.130e-02\n",
      " -6.900e-03 -3.200e-03 -4.620e-02  4.970e-02  2.610e-02  2.680e-02\n",
      " -3.100e-02 -1.361e-01 -6.200e-03  3.750e-02 -3.200e-02 -1.060e-02\n",
      "  5.340e-02 -1.870e-02  6.380e-02  9.400e-03  4.700e-03 -5.300e-02\n",
      "  9.300e-03 -8.700e-03  4.000e-04  4.930e-02 -6.296e-01  2.220e-02\n",
      "  1.900e-02  2.680e-02 -4.260e-02  5.700e-03 -1.683e-01  2.440e-02\n",
      " -2.130e-02 -1.810e-02  4.210e-02 -3.090e-02 -8.900e-03  3.200e-03\n",
      "  1.080e-02 -4.900e-03  2.580e-02  2.780e-02 -1.630e-02  2.000e-02\n",
      "  1.640e-02 -9.540e-02 -3.200e-03  4.300e-03  1.040e-02 -8.800e-03\n",
      "  7.000e-04  3.500e-02 -2.060e-02 -8.300e-03 -1.140e-02 -1.869e-01\n",
      "  2.580e-02  1.000e-03  8.500e-03  1.510e-02  2.125e-01  7.100e-03\n",
      "  3.190e-02 -4.820e-02  6.210e-02  6.260e-02  1.590e-02 -1.300e-03\n",
      "  8.700e-03  6.860e-02 -3.400e-03  2.380e-02 -4.520e-02 -1.980e-02\n",
      "  1.120e-02  1.090e-02 -1.022e-01 -2.720e-02  2.337e-01 -4.650e-02\n",
      "  1.592e-01 -4.070e-02 -1.029e-01 -4.870e-02 -6.760e-02  6.760e-02\n",
      " -3.280e-02  3.230e-02  7.700e-03  1.900e-02  1.700e-03 -2.974e-01\n",
      "  1.100e-03 -3.560e-02  6.930e-02 -4.800e-02 -8.210e-02 -6.440e-02\n",
      " -2.840e-02 -1.910e-02 -2.330e-02  3.530e-02 -4.630e-02  6.560e-02\n",
      "  1.900e-03 -2.120e-02 -3.090e-02 -3.534e-01 -3.090e-02  7.600e-03\n",
      " -4.190e-02  4.570e-02 -3.060e-02  3.570e-02  6.670e-02  3.659e-01\n",
      "  1.490e-02 -4.430e-02  6.800e-03 -3.780e-02  1.460e-02  2.150e-02\n",
      "  1.081e-01  1.240e-02 -4.370e-02 -4.300e-02  2.580e-02  2.130e-02\n",
      " -3.090e-02 -1.800e-03 -6.700e-03  1.720e-02  8.900e-03 -1.710e-02\n",
      "  2.750e-02 -5.180e-02 -1.840e-01 -1.300e-02 -2.410e-02  5.260e-02\n",
      " -2.800e-02  5.100e-03  1.630e-02 -1.650e-02  1.610e-02  1.237e-01\n",
      "  8.040e-02 -7.890e-02  3.860e-02 -3.892e-01  1.570e-02 -2.460e-02\n",
      "  4.770e-02 -4.500e-03 -2.140e-02  1.730e-02 -1.910e-02 -1.382e-01\n",
      " -1.110e-02  7.120e-02  1.514e-01  2.910e-02  5.550e-02 -3.900e-03\n",
      "  2.800e-03 -2.770e-02 -2.750e-02 -1.770e-02 -3.380e-02 -3.720e-02\n",
      "  2.071e-01  4.600e-02 -2.940e-02  4.350e-02 -1.690e-02 -1.210e-02\n",
      "  2.530e-02  1.980e-02  9.180e-02  1.930e-02  6.680e-02  2.880e-02\n",
      "  4.000e-03 -4.390e-02 -3.020e-02  6.400e-03  3.640e-02  5.430e-02\n",
      " -3.380e-02  1.590e-02  6.170e-02 -9.410e-02 -8.600e-03 -9.200e-03\n",
      "  3.000e-02 -2.410e-02 -3.500e-02 -6.210e-02  1.750e-02  3.740e-02\n",
      "  3.400e-03  3.440e-02  1.286e-01 -2.670e-02  1.861e-01  4.890e-02\n",
      " -3.200e-03  1.800e-02 -2.280e-02  2.414e-01 -9.350e-02  6.120e-02\n",
      " -2.090e-02  1.360e-02  3.920e-02 -1.350e-02 -2.530e-02  3.350e-02\n",
      "  9.500e-03  4.190e-02  7.600e-03  4.522e-01 -1.880e-02  2.330e-02\n",
      " -4.740e-02  1.590e-02 -9.000e-03  2.650e-02  3.360e-02  2.210e-02\n",
      "  4.720e-02  4.800e-03  9.620e-02  3.440e-02 -5.150e-02 -8.700e-03\n",
      " -9.800e-02 -2.880e-02  3.770e-02  2.020e-02 -2.979e-01 -3.870e-02\n",
      " -1.980e-02 -1.610e-02 -4.500e-03  8.700e-03 -3.870e-02  4.210e-02\n",
      "  3.830e-02  2.580e-02  6.900e-03 -2.980e-02 -1.980e-02 -1.520e-02\n",
      "  3.300e-03  7.500e-03  3.580e-02 -1.550e-02 -1.110e-02  7.600e-02\n",
      " -4.520e-02  6.970e-02  2.990e-02 -2.900e-03 -3.480e-02 -2.700e-02\n",
      "  3.510e-02  5.590e-02  5.910e-02  1.559e-01 -2.540e-02 -2.590e-02]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((n_vocabulary, n_embedding))\n",
    "for token, i in word_idx.items():\n",
    "    if i >= n_vocabulary:\n",
    "        continue\n",
    "    if token in embedding_model:\n",
    "        embedding_matrix[i] = embedding_model[token]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.zeros(n_embedding)\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 512, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 128)               165120    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 126)               16254     \n",
      "=================================================================\n",
      "Total params: 6,197,886\n",
      "Trainable params: 197,886\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def init_model():\n",
    "#     with MirroredStrategy().scope():\n",
    "        model = Sequential()\n",
    "\n",
    "#         model.add(Embedding(input_dim=n_vocabulary,\n",
    "#                             output_dim=n_embedding,\n",
    "#                             input_length=n_sequence))\n",
    "#         model.add(Dropout(.25))\n",
    "\n",
    "        model.add(Embedding(\n",
    "            input_dim=n_vocabulary,\n",
    "            output_dim=n_embedding,\n",
    "            embeddings_initializer=Constant(embedding_matrix),\n",
    "            input_length=n_sequence,\n",
    "            trainable=False\n",
    "        ))\n",
    "\n",
    "#         model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "#         model.add(MaxPooling1D(5))\n",
    "#         model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "#         model.add(MaxPooling1D(5))\n",
    "#         model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "#         model.add(GlobalMaxPooling1D())\n",
    "#         model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "#         model.add(Conv1D(64, 5, activation=\"relu\"))\n",
    "#         model.add(Dropout(.25))\n",
    "#         model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "#         model.add(Dropout(.25))\n",
    "#         model.add(Flatten())\n",
    "#         model.add(Dense(128))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(ReLU())\n",
    "#         model.add(Dropout(.25))\n",
    "#         model.add(Dense(128))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(ReLU())\n",
    "#         model.add(Dropout(.25))\n",
    "\n",
    "        model.add(GRU(128, dropout=.2))\n",
    "        model.add(Dense(128, activation=\"relu\"))\n",
    "        model.add(Dropout(.2))\n",
    "\n",
    "#         model.add(LSTM(128, dropout=.2))\n",
    "#         model.add(Dense(128, activation=\"relu\"))\n",
    "#         model.add(Dropout(.2))\n",
    "\n",
    "#         model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "#         model.add(Bidirectional(LSTM(128)))\n",
    "#         model.add(Dense(128, activation=\"relu\"))\n",
    "#         model.add(Dropout(.5))\n",
    "\n",
    "#         model.add(Dense(128, activation=\"relu\", input_shape=(n_vocabulary,)))\n",
    "#         model.add(Dropout(.5))\n",
    "\n",
    "        model.add(Dense(n_labels, activation=\"sigmoid\"))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "        return model\n",
    "\n",
    "init_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = None\n",
    "x_train, y_train = shuffle(sequences,\n",
    "                           train_labels,\n",
    "                           random_state=seed,\n",
    "                           n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0793 - val_loss: 0.0388\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0332 - val_loss: 0.0261\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0260 - val_loss: 0.0225\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0231 - val_loss: 0.0208\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0214 - val_loss: 0.0195\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0202 - val_loss: 0.0186\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0194 - val_loss: 0.0181\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0188 - val_loss: 0.0176\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0182 - val_loss: 0.0174\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0178 - val_loss: 0.0171\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0174 - val_loss: 0.0169\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0171 - val_loss: 0.0168\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0168 - val_loss: 0.0166\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0166 - val_loss: 0.0164\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0164 - val_loss: 0.0164\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0161 - val_loss: 0.0163\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0160 - val_loss: 0.0161\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0158 - val_loss: 0.0162\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0156 - val_loss: 0.0161\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0155 - val_loss: 0.0160\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 63s 67ms/step - loss: 0.0153 - val_loss: 0.0161\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0152 - val_loss: 0.0160\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0151 - val_loss: 0.0158\n",
      "Epoch 24/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0150 - val_loss: 0.0160\n",
      "Epoch 25/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0149 - val_loss: 0.0161\n",
      "Epoch 26/100\n",
      "936/937 [============================>.] - ETA: 0s - loss: 0.0148Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0148 - val_loss: 0.0160\n",
      "Epoch 00026: early stopping\n",
      "235/235 [==============================] - 8s 33ms/step\n",
      "{'accuracy': 0.6807438912517721, 'F1 (macro)': 0.5636952726331164, 'F1 (micro)': 0.8865603083016875, 'LRAP': 0.9550621471060059, 'NDCG': 0.9742116176736497}\n",
      "Epoch 1/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0809 - val_loss: 0.0392\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0332 - val_loss: 0.0262\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0262 - val_loss: 0.0228\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0233 - val_loss: 0.0208\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0216 - val_loss: 0.0200\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0204 - val_loss: 0.0189\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0196 - val_loss: 0.0183\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0189 - val_loss: 0.0179\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0184 - val_loss: 0.0177\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0180 - val_loss: 0.0173\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0176 - val_loss: 0.0172\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0173 - val_loss: 0.0170\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0170 - val_loss: 0.0167\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0168 - val_loss: 0.0167\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0165 - val_loss: 0.0166\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0163 - val_loss: 0.0165\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0161 - val_loss: 0.0163\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0159 - val_loss: 0.0166\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0158 - val_loss: 0.0162\n",
      "Epoch 20/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0157 - val_loss: 0.0161\n",
      "Epoch 21/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0155 - val_loss: 0.0163\n",
      "Epoch 22/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0153 - val_loss: 0.0162\n",
      "Epoch 23/100\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0152 - val_loss: 0.0161\n",
      "Epoch 24/100\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0151 - val_loss: 0.0162\n",
      "Epoch 25/100\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0150 - val_loss: 0.0161\n",
      "Epoch 26/100\n",
      "937/937 [==============================] - 60s 65ms/step - loss: 0.0149 - val_loss: 0.0161\n",
      "Epoch 27/100\n",
      "937/937 [==============================] - 60s 64ms/step - loss: 0.0148 - val_loss: 0.0161\n",
      "Epoch 28/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0147 - val_loss: 0.0161\n",
      "Epoch 29/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0146 - val_loss: 0.0159\n",
      "Epoch 30/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0145 - val_loss: 0.0163\n",
      "Epoch 31/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0145 - val_loss: 0.0161\n",
      "Epoch 32/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0144 - val_loss: 0.0159\n",
      "Epoch 33/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0143 - val_loss: 0.0160\n",
      "Epoch 34/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0143 - val_loss: 0.0158\n",
      "Epoch 35/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0141 - val_loss: 0.0162\n",
      "Epoch 36/100\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0141 - val_loss: 0.0161\n",
      "Epoch 37/100\n",
      "936/937 [============================>.] - ETA: 0s - loss: 0.0141Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 61s 65ms/step - loss: 0.0141 - val_loss: 0.0161\n",
      "Epoch 00037: early stopping\n",
      "235/235 [==============================] - 8s 33ms/step\n",
      "{'accuracy': 0.6841130848136102, 'F1 (macro)': 0.578419405340798, 'F1 (micro)': 0.8894977798761824, 'LRAP': 0.9559131994267945, 'NDCG': 0.9744004313435978}\n",
      "Epoch 1/100\n",
      "133/937 [===>..........................] - ETA: 44s - loss: 0.1735"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ca535fd425a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                         callbacks=[es])\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0my_pred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projappl/project_2002961/DL-2020-Shakespeare/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projappl/project_2002961/DL-2020-Shakespeare/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projappl/project_2002961/DL-2020-Shakespeare/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projappl/project_2002961/DL-2020-Shakespeare/venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projappl/project_2002961/DL-2020-Shakespeare/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projappl/project_2002961/DL-2020-Shakespeare/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projappl/project_2002961/DL-2020-Shakespeare/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projappl/project_2002961/DL-2020-Shakespeare/venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/projappl/project_2002961/DL-2020-Shakespeare/venv/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_scores = []\n",
    "batch_size = 256\n",
    "\n",
    "for train, val in kfold.split(x_train, y_train):\n",
    "    model = init_model()\n",
    "    es = EarlyStopping(patience=3, verbose=1, restore_best_weights=True)\n",
    "    history = model.fit(x_train[train],\n",
    "                        y_train[train],\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_train[val], y_train[val]),\n",
    "                        callbacks=[es])\n",
    "    \n",
    "    y_pred_prob = model.predict(x_train[val], batch_size=batch_size, verbose=1)\n",
    "    y_pred = np.round(y_pred_prob)\n",
    "    \n",
    "    scores = {}\n",
    "    scores[\"accuracy\"] = accuracy_score(y_train[val], y_pred)\n",
    "    scores[\"F1 (macro)\"] = f1_score(y_train[val], y_pred, average=\"macro\")\n",
    "    scores[\"F1 (micro)\"] = f1_score(y_train[val], y_pred, average=\"micro\")\n",
    "    scores[\"LRAP\"] = label_ranking_average_precision_score(y_train[val],\n",
    "                                                           y_pred_prob)\n",
    "    scores[\"NDCG\"] = ndcg_score(y_train[val], y_pred_prob)\n",
    "    cv_scores.append(scores)\n",
    "    print(scores)\n",
    "    \n",
    "cv_scores_df = pd.DataFrame(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and their means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cv_scores_df)\n",
    "print(cv_scores_df.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
