{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Text project\n",
    "\n",
    "**Due Thursday, May 22, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to news articles.  The corpus contains ~850K articles from Reuters.  The test set is about 10% of the articles. The data is unextracted in XML files.\n",
    "\n",
    "We're only giving you the code for downloading the data, and how to save the final model. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the project:\n",
    "\n",
    "- One document may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are documents that don't belong to any class, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "- You may use word-embeddings to get better results. For example, you were already using a smaller version of the GloVE  embeddings in exercise 4. Do note that these embeddings take a lot of memory. \n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, \\\n",
    "        GlobalMaxPooling1D, SpatialDropout1D, LSTM, GRU, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import data\n",
    "import preprocessing\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloads and extracts the data files into the `train` subdirectory.\n",
    "\n",
    "The files can be found in `train/`, and are named as `19970405.zip`, etc. You will have to manage the content of these zips to get the data. There is a readme which has links to further descriptions on the data.\n",
    "\n",
    "The class labels, or topics, can be found in the readme file called `train/codes.zip`.  The zip contains a file called \"topic_codes.txt\".  This file contains the special codes for the topics (about 130 of them), and the explanation - what each code means.\n",
    "\n",
    "The XML document files contain the article's headline, the main body text, and the list of topic labels assigned to each article.  You will have to extract the topics of each article from the XML.  For example: \n",
    "&lt;code code=\"C18\"&gt; refers to the topic \"OWNERSHIP CHANGES\" (like a corporate buyout).\n",
    "\n",
    "You should pre-process the XML to extract the words from the article: the &lt;headline&gt; element and the &lt;text&gt;.  You should not need any other parts of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299773,)\n",
      "(299773, 126)\n",
      "Typhoon Winnie kills 25 in Taiwan. A typhoon that packed high winds and torrential rain killed 25 people in Taiwan on Monday and Tuesday, with landslides bringing down buildings and floodwaters turning streets into rivers, officials said on Tuesday. The death toll has risen to 25, one missing, 16 seriously injured and 62 slightly hurt, the government's anti-typhoon centre said in a statement. Three houses totally collapsed and 37 partly collapsed, it said. State television showed several five-storey buildings in eastern Taipei that had sunk two stories into the ground. The Central Weather Bureau said late on Monday the danger had passed as Typhoon Winnie headed towards mainland China. Heavy torrential rain and strong winds triggered landslides in Taipei, destroying or damaging buildings and blocking traffic. \"The whole scene looks as if it has gone through an explosion,\" a state television reporter in the city said. Local authorities mobilised hundreds of rescue workers and soldiers to help evacuate residents, but several people remained trapped in their houses, state media said. Seven people were buried after landslides hit their house in north Taipei. Only one woman survived. Torrential rain swamped houses in low-lying areas of the capital. Many roads were flooded and scores of vehicles submerged. The typhoon, packing maximum sustained winds of 144 km per hour (89 miles per hour) and gusts of up to 180 kph (112 mph), forced government agencies, financial markets and schools to close on Monday. All domestic flights were cancelled but most international flights were running to schedule. Many roads in mountainous areas were impassable. Some 68,500 households suffered power cuts, officials said. All government offices, markets and schools and most of the island's traffic systems returned to normal operations on Tuesday, officials said.\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# data.extract_data(extraction_dir=\"train\", data_dir=\"data\", data_zip_name=\"reuters-training-corpus.zip\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_pickle(\"train/data.pkl\")\n",
    "except FileNotFoundError:\n",
    "    df = data.get_docs_labels(\"train/REUTERS_CORPUS_2\")\n",
    "    df.to_pickle(\"train/data.pkl\")\n",
    "\n",
    "docs = df[\"doc\"].values\n",
    "labels = np.array(df[\"labels\"].tolist())\n",
    "n_labels = len(data.CODEMAP)\n",
    "\n",
    "print(docs.shape)\n",
    "print(labels.shape)\n",
    "print(docs[-2])\n",
    "print(labels[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 47.6 GiB for an array with shape (299773,) and data type <U42585",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-391d495c5c5a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_to_prepocessed_docs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"rb\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m         \u001B[0mpreprocessed_docs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'train/preprocessed_docs_no_stop_words.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-391d495c5c5a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m         \u001B[0mpreprocessed_docs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mexcept\u001B[0m \u001B[0mFileNotFoundError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mpreprocessed_docs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpreprocessing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocess_corpus\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdocs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_to_prepocessed_docs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"wb\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0mpickle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdump\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpreprocessed_docs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\DL-2020-Shakespeare\\preprocessing.py\u001B[0m in \u001B[0;36mpreprocess_corpus\u001B[1;34m(corpus)\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mPool\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_cores\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpool\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m         \u001B[0msplit\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpool\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mworker\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msplit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mconcatenate\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 47.6 GiB for an array with shape (299773,) and data type <U42585"
     ]
    }
   ],
   "source": [
    "path_to_prepocessed_docs = \"train/preprocessed_docs.pkl\"\n",
    "try:\n",
    "    with open(path_to_prepocessed_docs, \"rb\") as f:\n",
    "        preprocessed_docs = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    preprocessed_docs = preprocessing.preprocess_corpus(docs)\n",
    "    with open(path_to_prepocessed_docs, \"wb\") as f:\n",
    "        pickle.dump(preprocessed_docs, f)\n",
    "\n",
    "preprocessed_docs = [s.split() for s in preprocessed_docs]\n",
    "\n",
    "print(\" \".join(preprocessed_docs[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_embedding = 100\n",
    "\n",
    "try:\n",
    "    w2v_model = Word2Vec.load(\"train/w2v.model\")\n",
    "except:\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=preprocessed_docs,\n",
    "        size=n_embedding, window=5,\n",
    "        workers=10,\n",
    "        min_count=1\n",
    "    )\n",
    "    w2v_model.save(\"train/w2v.model\")\n",
    "\n",
    "print(len(list(w2v_model.wv.vocab)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_sequence = max([len(doc) for doc in preprocessed_docs])\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(preprocessed_docs)\n",
    "word_idx = tokenizer.word_index\n",
    "n_vocabulary = len(word_idx) + 1\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_docs)\n",
    "sequences = pad_sequences(sequences, maxlen=n_sequence, padding=\"post\")\n",
    "\n",
    "print(n_sequence)\n",
    "print(n_vocabulary)\n",
    "print(\" \".join(preprocessed_docs[1]))\n",
    "print(sequences[1])\n",
    "\n",
    "# n_docs = 50000\n",
    "# n_vocabulary = 5000\n",
    "\n",
    "# x_train = preprocessed_docs[:n_docs]\n",
    "# y_train = labels[:n_docs]\n",
    "#\n",
    "# # x_train = preprocessed_docs\n",
    "# # y_train = labels\n",
    "\n",
    "# tokenizer = Tokenizer(filters=\"\", num_words=n_vocabulary)\n",
    "# tokenizer.fit_on_texts(x_train)\n",
    "# # n_vocabulary = len(tokenizer.word_index) + 1\n",
    "# x_train = tokenizer.texts_to_sequences(x_train)\n",
    "# n_max_sequence = len(max(x_train, key=len))\n",
    "# x_train = pad_sequences(x_train, maxlen=n_max_sequence, padding=\"post\")\n",
    "# x_train = np.array(x_train)\n",
    "\n",
    "# print(n_vocabulary)\n",
    "# print(n_max_sequence)\n",
    "# print(x_train.shape)\n",
    "# print(preprocessed_docs[1])\n",
    "# print(x_train[1])\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=.1, random_state=seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    embedding_matrix = np.load(\"train/embedding_matrix.npy\")\n",
    "except:\n",
    "    embedding_matrix = np.zeros((n_vocabulary, n_embedding))\n",
    "    for token, i in word_idx.items():\n",
    "        if token in w2v_model:\n",
    "            embedding_matrix[i] = w2v_model[token]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.zeros(n_embedding)\n",
    "    np.save(\"train/embedding_matrix.npy\", embedding_matrix)\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# embedding_idx = {}\n",
    "# for doc in preprocessed_docs:\n",
    "#     for token in doc:\n",
    "#         if token in w2v_model:\n",
    "#             embedding_idx[token] = w2v_model[token]\n",
    "#         else:\n",
    "#             embedding_idx[token] = np.zeros(n_embedding)\n",
    "\n",
    "# embedding_matrix = np.zeros((n_vocabulary, n_embedding))\n",
    "# for word, i in word_idx.items():\n",
    "#     embedding = embedding_idx.get(word)\n",
    "#     if embedding is not None:\n",
    "#         embedding_matrix[i] = embedding\n",
    "\n",
    "# print(embedding_matrix.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    n_vocabulary,\n",
    "    n_embedding,\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    input_length=n_sequence,\n",
    "    trainable=False\n",
    "))\n",
    "# model.add(Embedding(n_vocabulary, n_embedding, input_length=n_sequence))\n",
    "\n",
    "# model.add(GRU(32, dropout=.2, recurrent_dropout=.2))\n",
    "\n",
    "# model.add(LSTM(150, dropout=.5, return_sequences=True))\n",
    "model.add(LSTM(100, dropout=.5))\n",
    "\n",
    "# model.add(Conv1D(filters=100, kernel_size=3, activation=\"relu\"))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(100, activation=\"relu\"))\n",
    "# model.add(Dropout(.5))\n",
    "\n",
    "# model.add(SpatialDropout1D(.2))\n",
    "# model.add(LSTM(150, dropout=.2))\n",
    "\n",
    "# model.add(Dropout(.2))\n",
    "# model.add(Conv1D(filters=n_hidden, kernel_size=50, activation='relu', strides=1))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(n_hidden, activation=\"relu\"))\n",
    "# model.add(Dropout(.2))\n",
    "\n",
    "model.add(Dense(n_labels, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    sequences,\n",
    "    labels,\n",
    "    test_size=.1,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "es = EarlyStopping(patience=5, verbose=1, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=.1, callbacks=[es]\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "print(f\"test f1: {f1}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300821\n"
     ]
    }
   ],
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9271\n",
      "300822\n",
      "decision of the eea joint committee no -num- of -num- october -num- amend protocol -num- to the eea agreement on cooperation in specific field outside the -num- freedom decision of the eea joint committee no -num- of -num- january -num- amend annex ii technical regulation standard testing and certification to the eea agreement decision of the eea joint committee no -num- of -num- february -num- amend annex vi social security to the eea agreement end of document\n",
      "[347   5   2 ...   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "n_sequence = max([len(doc) for doc in preprocessed_docs])\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(preprocessed_docs)\n",
    "word_idx = tokenizer.word_index\n",
    "n_vocabulary = len(word_idx) + 1\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_docs)\n",
    "sequences = pad_sequences(sequences, maxlen=n_sequence, padding=\"post\")\n",
    "\n",
    "print(n_sequence)\n",
    "print(n_vocabulary)\n",
    "print(\" \".join(preprocessed_docs[1]))\n",
    "print(sequences[1])\n",
    "\n",
    "# n_docs = 50000\n",
    "# n_vocabulary = 5000\n",
    "\n",
    "# x_train = preprocessed_docs[:n_docs]\n",
    "# y_train = labels[:n_docs]\n",
    "#\n",
    "# # x_train = preprocessed_docs\n",
    "# # y_train = labels\n",
    "\n",
    "# tokenizer = Tokenizer(filters=\"\", num_words=n_vocabulary)\n",
    "# tokenizer.fit_on_texts(x_train)\n",
    "# # n_vocabulary = len(tokenizer.word_index) + 1\n",
    "# x_train = tokenizer.texts_to_sequences(x_train)\n",
    "# n_max_sequence = len(max(x_train, key=len))\n",
    "# x_train = pad_sequences(x_train, maxlen=n_max_sequence, padding=\"post\")\n",
    "# x_train = np.array(x_train)\n",
    "\n",
    "# print(n_vocabulary)\n",
    "# print(n_max_sequence)\n",
    "# print(x_train.shape)\n",
    "# print(preprocessed_docs[1])\n",
    "# print(x_train[1])\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300822, 100)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    embedding_matrix = np.load(\"train/embedding_matrix.npy\")\n",
    "except:\n",
    "    embedding_matrix = np.zeros((n_vocabulary, n_embedding))\n",
    "    for token, i in word_idx.items():\n",
    "        if token in w2v_model:\n",
    "            embedding_matrix[i] = w2v_model[token]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.zeros(n_embedding)\n",
    "    np.save(\"train/embedding_matrix.npy\", embedding_matrix)\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# embedding_idx = {}\n",
    "# for doc in preprocessed_docs:\n",
    "#     for token in doc:\n",
    "#         if token in w2v_model:\n",
    "#             embedding_idx[token] = w2v_model[token]\n",
    "#         else:\n",
    "#             embedding_idx[token] = np.zeros(n_embedding)\n",
    "\n",
    "# embedding_matrix = np.zeros((n_vocabulary, n_embedding))\n",
    "# for word, i in word_idx.items():\n",
    "#     embedding = embedding_idx.get(word)\n",
    "#     if embedding is not None:\n",
    "#         embedding_matrix[i] = embedding\n",
    "\n",
    "# print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 9271, 100)         30082200  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 126)               12726     \n",
      "=================================================================\n",
      "Total params: 30,175,326\n",
      "Trainable params: 93,126\n",
      "Non-trainable params: 30,082,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    n_vocabulary,\n",
    "    n_embedding,\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    input_length=n_sequence,\n",
    "    trainable=False\n",
    "))\n",
    "# model.add(Embedding(n_vocabulary, n_embedding, input_length=n_sequence))\n",
    "\n",
    "# model.add(GRU(32, dropout=.2, recurrent_dropout=.2))\n",
    "\n",
    "# model.add(LSTM(150, dropout=.5, return_sequences=True))\n",
    "model.add(LSTM(100, dropout=.5))\n",
    "\n",
    "# model.add(Conv1D(filters=100, kernel_size=3, activation=\"relu\"))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(100, activation=\"relu\"))\n",
    "# model.add(Dropout(.5))\n",
    "\n",
    "# model.add(SpatialDropout1D(.2))\n",
    "# model.add(LSTM(150, dropout=.2))\n",
    "\n",
    "# model.add(Dropout(.2))\n",
    "# model.add(Conv1D(filters=n_hidden, kernel_size=50, activation='relu', strides=1))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(n_hidden, activation=\"relu\"))\n",
    "# model.add(Dropout(.2))\n",
    "\n",
    "model.add(Dense(n_labels, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    sequences,\n",
    "    labels,\n",
    "    test_size=.1,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "es = EarlyStopping(patience=5, verbose=1, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=.1, callbacks=[es]\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "print(f\"test f1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".\n",
    "\n",
    "## Download test set\n",
    "\n",
    "The testset will be made available during the last week before the deadline and can be downloaded in the same way as the training set.\n",
    "\n",
    "## Predict for test set\n",
    "\n",
    "You will be asked to return your predictions a separate test set.  These should be returned as a matrix with one row for each test article.  Each row contains a binary prediction for each label, 1 if it's present in the image, and 0 if not. The order of the labels is the order of the label (topic) codes.\n",
    "\n",
    "An example row could like like this if your system predicts the presense of the second and fourth topic:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0 ...\n",
    "    \n",
    "If you have the matrix prepared in `y` you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}