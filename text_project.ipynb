{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Text project\n",
    "\n",
    "**Due Thursday, May 22, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to news articles.  The corpus contains ~850K articles from Reuters.  The test set is about 10% of the articles. The data is unextracted in XML files.\n",
    "\n",
    "We're only giving you the code for downloading the data, and how to save the final model. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the project:\n",
    "\n",
    "- One document may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are documents that don't belong to any class, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "- You may use word-embeddings to get better results. For example, you were already using a smaller version of the GloVE  embeddings in exercise 4. Do note that these embeddings take a lot of memory. \n",
    "- In the exercises we used e.g., `torchvision.datasets.MNIST` to handle the loading of the data in suitable batches. Here, you need to handle the dataloading yourself.  The easiest way is probably to create a custom `Dataset`. [See for example here for a tutorial](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import reuters\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, \\\n",
    "        GlobalMaxPooling1D, SpatialDropout1D, LSTM, GRU, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import data\n",
    "import preprocessing\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloads and extracts the data files into the `train` subdirectory.\n",
    "\n",
    "The files can be found in `train/`, and are named as `19970405.zip`, etc. You will have to manage the content of these zips to get the data. There is a readme which has links to further descriptions on the data.\n",
    "\n",
    "The class labels, or topics, can be found in the readme file called `train/codes.zip`.  The zip contains a file called \"topic_codes.txt\".  This file contains the special codes for the topics (about 130 of them), and the explanation - what each code means.\n",
    "\n",
    "The XML document files contain the article's headline, the main body text, and the list of topic labels assigned to each article.  You will have to extract the topics of each article from the XML.  For example: \n",
    "&lt;code code=\"C18\"&gt; refers to the topic \"OWNERSHIP CHANGES\" (like a corporate buyout).\n",
    "\n",
    "You should pre-process the XML to extract the words from the article: the &lt;headline&gt; element and the &lt;text&gt;.  You should not need any other parts of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299773\n",
      "(299773, 126)\n",
      "India must allow free imports and exports of gold and liberalise trade in the yellow metal to boost industrial and employment growth, senior government officials said on Saturday. \"The major objective of the new gold policy should perhaps be to recognise the importance of gold in the Indian economic system...,\" Y.V. Reddy, deputy governor of the Reserve Bank of India (RBI), told a gold seminar in the Indian capital. \"...(The policy) should enable gold to play a transparent and positive role in the industrial development, employment and export sectors of the economy,\" Reddy said. Reddy said India should allow free imports and exports of gold and give up its present system of trade through designated agencies. Currently state-run State Bank of India and MMTC Ltd are the only entities permitted to import gold. \"Free import under open general licence and free exports are pre-conditions for capturing world markets,\" Reddy said. He said most South Asian nations had liberalised gold imports, charging nominal duties. \"This development makes it imperative that we review our policies urgently.\" India is the world's largest consumer of gold, although nearly a third of its 900 million people live below the poverty line with the average per-capita income at around $330 a year. The country's annual consumption of gold is estimated at around 500 tonnes, 80 percent of it imported. India's main demand is for gold jewellery, given and used at an estimated 10 million weddings each year, which absorbs nearly 300 tonnes of the metal annually, bullion dealers say. P.P. Prabhu, the highest-ranking civil servant in the Commerce Ministry, told the seminar organised by the World Gold Council that much progress had been made in liberalising gold trade since 1991 when India launched an ambitious economic reforms programme. \"(But) of course the regime is still not very open,\" Prabhu added. He said smuggled gold currently accounted for only about 100-120 tonnes per year after India allowed non-resident Indians to bring in up to five kgs of gold each time they come to the country, subject to a certain duty. The limit was raised to 10 kgs in 1997. \"There is a significant reduction compared to early days when the percentage of smuggled gold was estimated to be more than 50 percent of the total consumption,\" Prabhu added. He said India's exports of gold jewellery had doubled since 1992/93 but there was still great potential for raising the country's share in the world jewellery trade. \"Although this sector has shown consistent growth, still, the potential is very much more especially considering the reservoir of talented craftsmen in India,\" Prabhu said. He said India's exports of gold and silver jewellery were worth $600 million annually against world trade of about $20 billion. India was considering ways to promote gold jewellery exports by allowing more agencies to import, stock and sell gold in domestic markets, Prabhu said. He said the RBI had already laid down guidelines for nominating more banks to import and sell gold and soon a number of new agencies would be permitted to import gold.\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# data.extract_data(extraction_dir=\"train\", data_dir=\"data\", data_zip_name=\"reuters-training-corpus.zip\")\n",
    "\n",
    "try:\n",
    "    with open(\"train/docs.pkl\", \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "    labels = np.load(\"train/labels.npy\")\n",
    "except:\n",
    "    docs, labels = data.get_docs_labels(\"train/REUTERS_CORPUS_2\")\n",
    "    with open(\"train/docs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(docs, f)\n",
    "    np.save(\"train/labels.npy\", labels)\n",
    "\n",
    "n_labels = len(labels[0])\n",
    "\n",
    "print(len(docs))\n",
    "print(labels.shape)\n",
    "\n",
    "print(docs[-2])\n",
    "print(labels[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india must allow free import and export of gold and liberalise trade in the yellow metal to boost industrial and employment growth senior government official say on saturday the major objective of the new gold policy should perhaps be to recognise the importance of gold in the indian economic system y.v. reddy deputy governor of the reserve bank of india rbi tell a gold seminar in the indian capital the policy should enable gold to play a transparent and positive role in the industrial development employment and export sector of the economy reddy say reddy say india should allow free import and export of gold and give up -pron- present system of trade through designate agency currently state run state bank of india and mmtc ltd be the only entity permit to import gold free import under open general licence and free export be pre condition for capture world market reddy say -pron- say most south asian nation have liberalise gold import charge nominal duty this development make -pron- imperative that -pron- review -pron- policy urgently india be the world 's large consumer of gold although nearly a third of -pron- -num- -num- people live below the poverty line with the average per capita income at around $ -num- a year the country 's annual consumption of gold be estimate at around -num- tonne -num- percent of -pron- import india 's main demand be for gold jewellery give and use at an estimate -num- -num- wedding each year which absorb nearly -num- tonne of the metal annually bullion dealer say p.p. prabhu the highest rank civil servant in the commerce ministry tell the seminar organise by the world gold council that much progress have be make in liberalise gold trade since -num- when india launch an ambitious economic reform programme but of course the regime be still not very open prabhu add -pron- say smuggle gold currently account for only about -num- -num- tonne per year after india allow non resident indians to bring in up to -num- kg of gold each time -pron- come to the country subject to a certain duty the limit be raise to -num- kg in -num- there be a significant reduction compare to early day when the percentage of smuggle gold be estimate to be more than -num- percent of the total consumption prabhu add -pron- say india 's export of gold jewellery have double since -num- but there be still great potential for raise the country 's share in the world jewellery trade although this sector have show consistent growth still the potential be very much more especially consider the reservoir of talented craftsman in india prabhu say -pron- say india 's export of gold and silver jewellery be worth $ -num- -num- annually against world trade of about $ -num- -num- india be consider way to promote gold jewellery export by allow more agency to import stock and sell gold in domestic market prabhu say -pron- say the rbi have already lay down guideline for nominate more bank to import and sell gold and soon a number of new agency would be permit to import gold\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"train/preprocessed_docs.pkl\", \"rb\") as f:\n",
    "        preprocessed_docs = pickle.load(f)\n",
    "except:\n",
    "    preprocessed_docs = preprocessing.preprocess_corpus(docs)\n",
    "    with open(\"train/preprocessed_docs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(preprocessed_docs, f)\n",
    "\n",
    "preprocessed_docs = [s.split() for s in preprocessed_docs]\n",
    "\n",
    "print(\" \".join(preprocessed_docs[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300821\n"
     ]
    }
   ],
   "source": [
    "n_embedding = 100\n",
    "\n",
    "try:\n",
    "    w2v_model = Word2Vec.load(\"train/w2v.model\")\n",
    "except:\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=preprocessed_docs,\n",
    "        size=n_embedding, window=5,\n",
    "        workers=10,\n",
    "        min_count=1\n",
    "    )\n",
    "    w2v_model.save(\"train/w2v.model\")\n",
    "\n",
    "print(len(list(w2v_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9271\n",
      "300822\n",
      "decision of the eea joint committee no -num- of -num- october -num- amend protocol -num- to the eea agreement on cooperation in specific field outside the -num- freedom decision of the eea joint committee no -num- of -num- january -num- amend annex ii technical regulation standard testing and certification to the eea agreement decision of the eea joint committee no -num- of -num- february -num- amend annex vi social security to the eea agreement end of document\n",
      "[347   5   2 ...   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "n_sequence = max([len(doc) for doc in preprocessed_docs])\n",
    "\n",
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(preprocessed_docs)\n",
    "word_idx = tokenizer.word_index\n",
    "n_vocabulary = len(word_idx) + 1\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_docs)\n",
    "sequences = pad_sequences(sequences, maxlen=n_sequence, padding=\"post\")\n",
    "\n",
    "print(n_sequence)\n",
    "print(n_vocabulary)\n",
    "print(\" \".join(preprocessed_docs[1]))\n",
    "print(sequences[1])\n",
    "\n",
    "# n_docs = 50000\n",
    "# n_vocabulary = 5000\n",
    "\n",
    "# x_train = preprocessed_docs[:n_docs]\n",
    "# y_train = labels[:n_docs]\n",
    "#\n",
    "# # x_train = preprocessed_docs\n",
    "# # y_train = labels\n",
    "\n",
    "# tokenizer = Tokenizer(filters=\"\", num_words=n_vocabulary)\n",
    "# tokenizer.fit_on_texts(x_train)\n",
    "# # n_vocabulary = len(tokenizer.word_index) + 1\n",
    "# x_train = tokenizer.texts_to_sequences(x_train)\n",
    "# n_max_sequence = len(max(x_train, key=len))\n",
    "# x_train = pad_sequences(x_train, maxlen=n_max_sequence, padding=\"post\")\n",
    "# x_train = np.array(x_train)\n",
    "\n",
    "# print(n_vocabulary)\n",
    "# print(n_max_sequence)\n",
    "# print(x_train.shape)\n",
    "# print(preprocessed_docs[1])\n",
    "# print(x_train[1])\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300822, 100)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    embedding_matrix = np.load(\"train/embedding_matrix.npy\")\n",
    "except:\n",
    "    embedding_matrix = np.zeros((n_vocabulary, n_embedding))\n",
    "    for token, i in word_idx.items():\n",
    "        if token in w2v_model:\n",
    "            embedding_matrix[i] = w2v_model[token]\n",
    "        else:\n",
    "            embedding_matrix[i] = np.zeros(n_embedding)\n",
    "    np.save(\"train/embedding_matrix.npy\", embedding_matrix)\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "# embedding_idx = {}\n",
    "# for doc in preprocessed_docs:\n",
    "#     for token in doc:\n",
    "#         if token in w2v_model:\n",
    "#             embedding_idx[token] = w2v_model[token]\n",
    "#         else:\n",
    "#             embedding_idx[token] = np.zeros(n_embedding)\n",
    "\n",
    "# embedding_matrix = np.zeros((n_vocabulary, n_embedding))\n",
    "# for word, i in word_idx.items():\n",
    "#     embedding = embedding_idx.get(word)\n",
    "#     if embedding is not None:\n",
    "#         embedding_matrix[i] = embedding\n",
    "\n",
    "# print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 9271, 100)         30082200  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 126)               12726     \n",
      "=================================================================\n",
      "Total params: 30,175,326\n",
      "Trainable params: 93,126\n",
      "Non-trainable params: 30,082,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    n_vocabulary,\n",
    "    n_embedding,\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    input_length=n_sequence,\n",
    "    trainable=False\n",
    "))\n",
    "# model.add(Embedding(n_vocabulary, n_embedding, input_length=n_sequence))\n",
    "\n",
    "# model.add(GRU(32, dropout=.2, recurrent_dropout=.2))\n",
    "\n",
    "# model.add(LSTM(150, dropout=.5, return_sequences=True))\n",
    "model.add(LSTM(100, dropout=.5))\n",
    "\n",
    "# model.add(Conv1D(filters=100, kernel_size=3, activation=\"relu\"))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(100, activation=\"relu\"))\n",
    "# model.add(Dropout(.5))\n",
    "\n",
    "# model.add(SpatialDropout1D(.2))\n",
    "# model.add(LSTM(150, dropout=.2))\n",
    "\n",
    "# model.add(Dropout(.2))\n",
    "# model.add(Conv1D(filters=n_hidden, kernel_size=50, activation='relu', strides=1))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(n_hidden, activation=\"relu\"))\n",
    "# model.add(Dropout(.2))\n",
    "\n",
    "model.add(Dense(n_labels, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    sequences,\n",
    "    labels,\n",
    "    test_size=.1,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "es = EarlyStopping(patience=5, verbose=1, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=.1, callbacks=[es]\n",
    ")\n",
    "\n",
    "y_pred = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "print(f\"test f1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "It might be useful to save your model if you want to continue your work later, or use it for inference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".\n",
    "\n",
    "## Download test set\n",
    "\n",
    "The testset will be made available during the last week before the deadline and can be downloaded in the same way as the training set.\n",
    "\n",
    "## Predict for test set\n",
    "\n",
    "You will be asked to return your predictions a separate test set.  These should be returned as a matrix with one row for each test article.  Each row contains a binary prediction for each label, 1 if it's present in the image, and 0 if not. The order of the labels is the order of the label (topic) codes.\n",
    "\n",
    "An example row could like like this if your system predicts the presense of the second and fourth topic:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0 ...\n",
    "    \n",
    "If you have the matrix prepared in `y` you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
