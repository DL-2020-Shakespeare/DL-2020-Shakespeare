{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import random as rn\n",
    "import warnings\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import accuracy_score, f1_score, hamming_loss, \\\n",
    "        average_precision_score, ndcg_score, \\\n",
    "        label_ranking_average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.distribute import MirroredStrategy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant, GlorotUniform\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, \\\n",
    "        GlobalMaxPooling1D, SpatialDropout1D, LSTM, GRU, Flatten, MaxPooling1D, \\\n",
    "        BatchNormalization, ReLU, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import data\n",
    "import preprocessing\n",
    "\n",
    "seed = 42\n",
    "sns.set()\n",
    "\n",
    "def reset_seed():\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299773,)\n",
      "(299773, 126)\n",
      "Toronto stocks end higher after volatile session. CHANGE\t\t\t\t    CHANGE TSE\t  5900.37    +50.15   HI 5900.37\t    LO  5840.29 DJI\t  6611.05    +27.57   GOLD (LONDON)   US$350.00 +1.90 FTSE100    4248.10    -64.80   GOLD (NY-COMEX) US$354.80 +0.70 NIKKEI    17869.59   -133.81   LME CASH NICKEL US$7659   +99.0 CANDLR\t1.3883\t\t LME CASH ALUM   US$1602.0  -4.0 CAN 30-YR   107.41     -0.15   BRENT CRUDE     US$19.09  -0.27 --------------------MARKET COMMENT---------------------------- * Toronto stocks ended higher on Tuesday, buoyed by strength in golds and banking * Computer problems due to heavy trading in Bre-X Minerals hampered session * 84 million shares traded Toronto's key stock index ended higher on Tuesday as the saga of Bre-X Minerals Ltd and its Indonesian gold find continued to dominate Canada's biggest stock market. The TSE 300 Index climbed 50.15 points to close at 5900.37 in heavy turnover of 84.07 million shares worth C$1.4 billion. But the overall market was mixed with declining issues narrowly outpacing advances 476 to 464. 298 issues were flat. Frantic trading in Bre-X collapsed the TSE's computer trading system earlier in the day, forcing the exchange to halt trading in the stock before the market closed. Shares in the Calgary-based gold prospector were halted for a statement by the company this morning. When it resumed, a whopping 7.7 million shares changed hands in the first 22 minutes of trading before the system crashed. Bre-X closed up 1.35 at 3.85. It was the first time Bre-X traded since investors lopped nearly C$3 billion off its stock market value last Thursday. TSE officials said the trading problems were due to old technology which will be replaced. On the Montreal Exchange, Bre-X closed up 0.81 at 3.50 on 9.8 million shares. Analysts predicted more volatility for Bre-X shares this week. \"The question of what Bre-X will release over the next few days will be important to the market,\" said Josef Schachter, of Schachter Asset Management Inc. The gold sector rose nearly 136 points, leading 12 of 14 sub-indices higher. Other strong groups included financial services, consumer products, energy and transportation. The TSE posted minor losses in forestry and real estate. --- HOT STOCKS --- * Among bank shares, Bank of Nova Scotia rose 0.65 to 51.50 on 2.1 million shares, while Canadian Imperial Bank of Commerce added 0.50 to 31.80 on 2.1 million shares. ((Reuters Toronto Bureau (416) 941-8100))\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# data.extract_data(extraction_dir=\"train\",\n",
    "#                   data_dir=\"data\",\n",
    "#                   data_zip_name=\"reuters-training-corpus.zip\")\n",
    "\n",
    "train_df = pd.read_pickle(\"train/data.pkl\")\n",
    "\n",
    "# train_df = data.get_docs_labels(\"train/REUTERS_CORPUS_2\")\n",
    "# train_df.to_pickle(\"train/data.pkl\")\n",
    "\n",
    "train_docs = train_df[\"doc\"].values\n",
    "train_labels = np.array(train_df[\"labels\"].tolist())\n",
    "n_labels = len(data.CODEMAP)\n",
    "\n",
    "print(train_docs.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_docs[2])\n",
    "print(train_labels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto stocks end higher after volatile session . CHANGE CHANGE TSE 5900.37 +50.15 HI 5900.37 LO 5840.29 DJI 6611.05 +27.57 GOLD ( LONDON ) US$ 350.00 +1.90 FTSE100 4248.10 -64.80 GOLD ( NY - COMEX ) US$ 354.80 +0.70 NIKKEI 17869.59 -133.81 LME CASH NICKEL US$ 7659 +99.0 CANDLR 1.3883 LME CASH ALUM US$ 1602.0 -4.0 CAN 30-YR 107.41 -0.15 BRENT CRUDE US$ 19.09 -0.27 --------------------MARKET COMMENT---------------------------- * Toronto stocks ended higher on Tuesday , buoyed by strength in golds and banking * Computer problems due to heavy trading in Bre - X Minerals hampered session * 84 million shares traded Toronto 's key stock index ended higher on Tuesday as the saga of Bre - X Minerals Ltd and its Indonesian gold find continued to dominate Canada 's biggest stock market . The TSE 300 Index climbed 50.15 points to close at 5900.37 in heavy turnover of 84.07 million shares worth C$ 1.4 billion . But the overall market was mixed with declining issues narrowly outpacing advances 476 to 464 . 298 issues were flat . Frantic trading in Bre - X collapsed the TSE 's computer trading system earlier in the day , forcing the exchange to halt trading in the stock before the market closed . Shares in the Calgary - based gold prospector were halted for a statement by the company this morning . When it resumed , a whopping 7.7 million shares changed hands in the first 22 minutes of trading before the system crashed . Bre - X closed up 1.35 at 3.85 . It was the first time Bre - X traded since investors lopped nearly C$ 3 billion off its stock market value last Thursday . TSE officials said the trading problems were due to old technology which will be replaced . On the Montreal Exchange , Bre - X closed up 0.81 at 3.50 on 9.8 million shares . Analysts predicted more volatility for Bre - X shares this week . \" The question of what Bre - X will release over the next few days will be important to the market , \" said Josef Schachter , of Schachter Asset Management Inc. The gold sector rose nearly 136 points , leading 12 of 14 sub - indices higher . Other strong groups included financial services , consumer products , energy and transportation . The TSE posted minor losses in forestry and real estate . --- HOT STOCKS --- * Among bank shares , Bank of Nova Scotia rose 0.65 to 51.50 on 2.1 million shares , while Canadian Imperial Bank of Commerce added 0.50 to 31.80 on 2.1 million shares . ( ( Reuters Toronto Bureau ( 416 ) 941 - 8100 ) )\n"
     ]
    }
   ],
   "source": [
    "path_to_preprocessed_docs = \"train/preprocessed_docs.pkl\"\n",
    "# path_to_preprocessed_docs = \"train/preprocessed_docs_lemmatized_no_sw.pkl\"\n",
    "\n",
    "with open(path_to_preprocessed_docs, \"rb\") as f:\n",
    "    preprocessed_train_docs = pickle.load(f)\n",
    "\n",
    "# preprocessed_train_docs = preprocessing.preprocess_corpus(train_docs)\n",
    "# with open(path_to_preprocessed_docs, \"wb\") as f:\n",
    "#     pickle.dump(preprocessed_train_docs, f)\n",
    "\n",
    "print(preprocessed_train_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = preprocessed_train_docs\n",
    "n_vocabulary = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As token index sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=n_vocabulary, filters=\"\", lower=False)\n",
    "tokenizer.fit_on_texts(docs)\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "if n_vocabulary is None:\n",
    "    n_vocabulary = len(word_idx)\n",
    "\n",
    "print(n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "(299773, 768)\n",
      "[ 2112   217   137   145    52  2639   593     2  3714  3714  9473  8514\n",
      "  9051  4802    14  2981    13   587  4802    14  3274     7  3355    13\n",
      "   587 15304  1660  5594 16940   587  1660  5594   587 14783  1709 11671\n",
      " 15729 13077   587    38  2112   217   226   145    12    85     3  5774\n",
      "    23  1043     6 11672     9   809    38  3318   700   167     4   901\n",
      "   181     6  3495     7  1927  6603  9271   593    38  4214    31    75\n",
      "   418  2112    15   435   150   180   226   145    12    85    30     1\n",
      " 15801     5  3495     7  1927  6603   200     9    32  1780   441  1151\n",
      "   598     4  7599   622    15   742   150    48     2    17  9473   893\n",
      "   770  2361   129     4   187    18     6   901   810     5    31    75\n",
      "   652  1105  2115    59     2   103     1  1026    48    19  1192    24\n",
      "  3826   474  5532 19549  4384     4     2 15888   474    36  1131     2\n",
      "   181     6  3495     7  1927  4223     1  9473    15  1373   181   394\n",
      "   252     6     1   133     3  4645     1   364     4  3029   181     6\n",
      "     1   150   141     1    48   249     2   639     6     1  7969     7\n",
      "   235   441    36  4627    16     8   188    23     1    55    54   543\n",
      "     2  1746    27  4021     3     8  5741    31    75  1072  1536     6\n",
      "     1    68   357  1492     5   181   141     1   394  5917     2  3495\n",
      "     7  1927   249    49  6742    18  9474     2    78    19     1    68\n",
      "   132  3495     7  1927   418   172   283   882  1105   157    59   155\n",
      "    32   150    48   541    62    93     2  9473   239    11     1   181\n",
      "   700    36   167     4   563   990    45    33    28  3622     2   454\n",
      "     1  5766   405     3  3495     7  1927   249    49 11132    18  4540\n",
      "    12  6814    31    75     2   769  2067    73  2253    16  3495     7\n",
      "  1927    75    54    77     2    10    17  1421     5   313  3495     7\n",
      "  1927    33   954    69     1   142   467   293    33    28   870     4\n",
      "     1    48     3    10    11 13262     3     5  5333  1926   457    17\n",
      "   441   342   162   882  9128   129     3   481   260     5   365  4046\n",
      "     7  3346   145     2   739   265   907   965   246   488     3   835\n",
      "   386     3  1233     9  4519     2    17  9473  1244  4193   628     6\n",
      "  8186     9   774  2225     2   637  4898   637    38  2277   107    75\n",
      "     3    81     5  5960  7296   162  6743     4    12  3042    31    75\n",
      "     3   156   872  7726    81     5  2222   159  2087     4    12  3042\n",
      "    31    75     2    14    14   183  2112  1637    14  4582    13  1997\n",
      "     7  7324    13    13     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "n_sequence = 768\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "sequence_lengths = [len(s) for s in sequences]\n",
    "if n_sequence is None:\n",
    "    n_sequence = max(sequence_lengths)\n",
    "sequences = pad_sequences(sequences,\n",
    "                          maxlen=n_sequence,\n",
    "                          padding=\"post\",\n",
    "                          truncating=\"post\")\n",
    "\n",
    "print(n_sequence)\n",
    "print(sequences.shape)\n",
    "print(sequences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(sequence_lengths, bins=np.arange(0, 1525, 25), kde=False)\n",
    "plt.title(f\"Document lengths (n_vocabulary={n_vocabulary})\")\n",
    "plt.xlim(0, 1500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_term_matrix = tokenizer.texts_to_matrix(docs, mode=\"tfidf\")\n",
    "\n",
    "# print(doc_term_matrix.shape)\n",
    "# print(doc_term_matrix[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_embedding = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# reset_seed()\n",
    "\n",
    "# window = 5\n",
    "# w2v_path = f\"data/w2v_{window}_{n_embedding}.model\"\n",
    "\n",
    "# try:\n",
    "#     embedding_model = Word2Vec.load(w2v_path)\n",
    "# except:\n",
    "#     embedding_model = Word2Vec(sentences=[s.split() for s in docs],\n",
    "#                                size=n_embedding, \n",
    "#                                window=window,\n",
    "#                                min_count=5,\n",
    "#                                sg=1,\n",
    "#                                workers=cpu_count(),\n",
    "#                                seed=seed)\n",
    "#     embedding_model.save(w2v_path)\n",
    "\n",
    "# print(len(list(embedding_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    return data\n",
    "\n",
    "embedding_model = load_vectors(\"data/wiki-news-300d-1M.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 300)\n",
      "[ 8.970e-02  1.600e-02 -5.710e-02  4.050e-02 -6.960e-02 -1.237e-01\n",
      "  3.010e-02  2.480e-02 -3.030e-02  1.740e-02  6.300e-03  1.840e-02\n",
      "  2.170e-02 -2.570e-02  3.500e-02 -2.420e-02  2.900e-03  1.880e-02\n",
      " -5.700e-02  2.520e-02 -2.100e-02 -8.000e-04  3.600e-02 -7.290e-02\n",
      " -6.650e-02  9.890e-02  6.760e-02  8.520e-02 -8.900e-03  3.130e-02\n",
      " -6.900e-03 -3.200e-03 -4.620e-02  4.970e-02  2.610e-02  2.680e-02\n",
      " -3.100e-02 -1.361e-01 -6.200e-03  3.750e-02 -3.200e-02 -1.060e-02\n",
      "  5.340e-02 -1.870e-02  6.380e-02  9.400e-03  4.700e-03 -5.300e-02\n",
      "  9.300e-03 -8.700e-03  4.000e-04  4.930e-02 -6.296e-01  2.220e-02\n",
      "  1.900e-02  2.680e-02 -4.260e-02  5.700e-03 -1.683e-01  2.440e-02\n",
      " -2.130e-02 -1.810e-02  4.210e-02 -3.090e-02 -8.900e-03  3.200e-03\n",
      "  1.080e-02 -4.900e-03  2.580e-02  2.780e-02 -1.630e-02  2.000e-02\n",
      "  1.640e-02 -9.540e-02 -3.200e-03  4.300e-03  1.040e-02 -8.800e-03\n",
      "  7.000e-04  3.500e-02 -2.060e-02 -8.300e-03 -1.140e-02 -1.869e-01\n",
      "  2.580e-02  1.000e-03  8.500e-03  1.510e-02  2.125e-01  7.100e-03\n",
      "  3.190e-02 -4.820e-02  6.210e-02  6.260e-02  1.590e-02 -1.300e-03\n",
      "  8.700e-03  6.860e-02 -3.400e-03  2.380e-02 -4.520e-02 -1.980e-02\n",
      "  1.120e-02  1.090e-02 -1.022e-01 -2.720e-02  2.337e-01 -4.650e-02\n",
      "  1.592e-01 -4.070e-02 -1.029e-01 -4.870e-02 -6.760e-02  6.760e-02\n",
      " -3.280e-02  3.230e-02  7.700e-03  1.900e-02  1.700e-03 -2.974e-01\n",
      "  1.100e-03 -3.560e-02  6.930e-02 -4.800e-02 -8.210e-02 -6.440e-02\n",
      " -2.840e-02 -1.910e-02 -2.330e-02  3.530e-02 -4.630e-02  6.560e-02\n",
      "  1.900e-03 -2.120e-02 -3.090e-02 -3.534e-01 -3.090e-02  7.600e-03\n",
      " -4.190e-02  4.570e-02 -3.060e-02  3.570e-02  6.670e-02  3.659e-01\n",
      "  1.490e-02 -4.430e-02  6.800e-03 -3.780e-02  1.460e-02  2.150e-02\n",
      "  1.081e-01  1.240e-02 -4.370e-02 -4.300e-02  2.580e-02  2.130e-02\n",
      " -3.090e-02 -1.800e-03 -6.700e-03  1.720e-02  8.900e-03 -1.710e-02\n",
      "  2.750e-02 -5.180e-02 -1.840e-01 -1.300e-02 -2.410e-02  5.260e-02\n",
      " -2.800e-02  5.100e-03  1.630e-02 -1.650e-02  1.610e-02  1.237e-01\n",
      "  8.040e-02 -7.890e-02  3.860e-02 -3.892e-01  1.570e-02 -2.460e-02\n",
      "  4.770e-02 -4.500e-03 -2.140e-02  1.730e-02 -1.910e-02 -1.382e-01\n",
      " -1.110e-02  7.120e-02  1.514e-01  2.910e-02  5.550e-02 -3.900e-03\n",
      "  2.800e-03 -2.770e-02 -2.750e-02 -1.770e-02 -3.380e-02 -3.720e-02\n",
      "  2.071e-01  4.600e-02 -2.940e-02  4.350e-02 -1.690e-02 -1.210e-02\n",
      "  2.530e-02  1.980e-02  9.180e-02  1.930e-02  6.680e-02  2.880e-02\n",
      "  4.000e-03 -4.390e-02 -3.020e-02  6.400e-03  3.640e-02  5.430e-02\n",
      " -3.380e-02  1.590e-02  6.170e-02 -9.410e-02 -8.600e-03 -9.200e-03\n",
      "  3.000e-02 -2.410e-02 -3.500e-02 -6.210e-02  1.750e-02  3.740e-02\n",
      "  3.400e-03  3.440e-02  1.286e-01 -2.670e-02  1.861e-01  4.890e-02\n",
      " -3.200e-03  1.800e-02 -2.280e-02  2.414e-01 -9.350e-02  6.120e-02\n",
      " -2.090e-02  1.360e-02  3.920e-02 -1.350e-02 -2.530e-02  3.350e-02\n",
      "  9.500e-03  4.190e-02  7.600e-03  4.522e-01 -1.880e-02  2.330e-02\n",
      " -4.740e-02  1.590e-02 -9.000e-03  2.650e-02  3.360e-02  2.210e-02\n",
      "  4.720e-02  4.800e-03  9.620e-02  3.440e-02 -5.150e-02 -8.700e-03\n",
      " -9.800e-02 -2.880e-02  3.770e-02  2.020e-02 -2.979e-01 -3.870e-02\n",
      " -1.980e-02 -1.610e-02 -4.500e-03  8.700e-03 -3.870e-02  4.210e-02\n",
      "  3.830e-02  2.580e-02  6.900e-03 -2.980e-02 -1.980e-02 -1.520e-02\n",
      "  3.300e-03  7.500e-03  3.580e-02 -1.550e-02 -1.110e-02  7.600e-02\n",
      " -4.520e-02  6.970e-02  2.990e-02 -2.900e-03 -3.480e-02 -2.700e-02\n",
      "  3.510e-02  5.590e-02  5.910e-02  1.559e-01 -2.540e-02 -2.590e-02]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((n_vocabulary, n_embedding))\n",
    "for token, i in word_idx.items():\n",
    "    if i >= n_vocabulary:\n",
    "        continue\n",
    "    if token in embedding_model:\n",
    "        embedding_matrix[i] = embedding_model[token]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.zeros(n_embedding)\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 768, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 512)               1140736   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 126)               32382     \n",
      "=================================================================\n",
      "Total params: 7,304,446\n",
      "Trainable params: 1,304,446\n",
      "Non-trainable params: 6,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def init_model():\n",
    "    with MirroredStrategy().scope():\n",
    "        model = Sequential()\n",
    "\n",
    "#         model.add(Embedding(input_dim=n_vocabulary,\n",
    "#                             output_dim=n_embedding,\n",
    "#                             input_length=n_sequence))\n",
    "#         model.add(Dropout(.25))\n",
    "\n",
    "        model.add(Embedding(\n",
    "            input_dim=n_vocabulary,\n",
    "            output_dim=n_embedding,\n",
    "            embeddings_initializer=Constant(embedding_matrix),\n",
    "            input_length=n_sequence,\n",
    "            trainable=False\n",
    "        ))\n",
    "\n",
    "#         model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "#         model.add(MaxPooling1D(5))\n",
    "#         model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "#         model.add(MaxPooling1D(5))\n",
    "#         model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "#         model.add(GlobalMaxPooling1D())\n",
    "#         model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "#         model.add(Conv1D(64, 5, activation=\"relu\"))\n",
    "#         model.add(Dropout(.25))\n",
    "#         model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "#         model.add(Dropout(.25))\n",
    "#         model.add(Flatten())\n",
    "#         model.add(Dense(128))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(ReLU())\n",
    "#         model.add(Dropout(.25))\n",
    "#         model.add(Dense(128))\n",
    "#         model.add(BatchNormalization())\n",
    "#         model.add(ReLU())\n",
    "#         model.add(Dropout(.25))\n",
    "\n",
    "#         model.add(GRU(128, dropout=.2))\n",
    "#         model.add(Dense(128, activation=\"relu\"))\n",
    "#         model.add(Dropout(.2))\n",
    "\n",
    "#         model.add(LSTM(256, dropout=.2))\n",
    "#         model.add(Dense(256, activation=\"relu\"))\n",
    "#         model.add(Dropout(.2))\n",
    "\n",
    "        model.add(Bidirectional(LSTM(256, dropout=.25)))\n",
    "        model.add(Dense(256, activation=\"relu\"))\n",
    "        model.add(Dropout(.25))\n",
    "\n",
    "#         model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "#         model.add(Bidirectional(LSTM(128)))\n",
    "#         model.add(Dense(128, activation=\"relu\"))\n",
    "#         model.add(Dropout(.5))\n",
    "\n",
    "#         model.add(Dense(128, activation=\"relu\", input_shape=(n_vocabulary,)))\n",
    "#         model.add(Dropout(.5))\n",
    "\n",
    "        model.add(Dense(n_labels, activation=\"sigmoid\"))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "        return model\n",
    "\n",
    "init_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = None\n",
    "x_train, y_train = shuffle(sequences,\n",
    "                           train_labels,\n",
    "                           random_state=seed,\n",
    "                           n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0612INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "937/937 [==============================] - 125s 133ms/step - loss: 0.0612 - val_loss: 0.0333\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0301 - val_loss: 0.0241\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0240 - val_loss: 0.0211\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0213 - val_loss: 0.0194\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0197 - val_loss: 0.0184\n",
      "Epoch 6/100\n",
      "937/937 [==============================] - 121s 129ms/step - loss: 0.0185 - val_loss: 0.0174\n",
      "Epoch 7/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0177 - val_loss: 0.0170\n",
      "Epoch 8/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0171 - val_loss: 0.0165\n",
      "Epoch 9/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0165 - val_loss: 0.0162\n",
      "Epoch 10/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0160 - val_loss: 0.0160\n",
      "Epoch 11/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0155 - val_loss: 0.0159\n",
      "Epoch 12/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0151 - val_loss: 0.0157\n",
      "Epoch 13/100\n",
      "937/937 [==============================] - 121s 130ms/step - loss: 0.0148 - val_loss: 0.0157\n",
      "Epoch 14/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0144 - val_loss: 0.0157\n",
      "Epoch 15/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0141 - val_loss: 0.0157\n",
      "Epoch 16/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0139 - val_loss: 0.0156\n",
      "Epoch 17/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0135 - val_loss: 0.0156\n",
      "Epoch 18/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0132 - val_loss: 0.0157\n",
      "Epoch 19/100\n",
      "937/937 [==============================] - ETA: 0s - loss: 0.0130Restoring model weights from the end of the best epoch.\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0130 - val_loss: 0.0158\n",
      "Epoch 00019: early stopping\n",
      "235/235 [==============================] - 11s 45ms/step\n",
      "{'accuracy': 0.6901509465432407, 'F1 (macro)': 0.5815354925601058, 'F1 (micro)': 0.889585303688505, 'LRAP': 0.9572761316883125, 'NDCG': 0.9752919687044291}\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "937/937 [==============================] - 125s 133ms/step - loss: 0.0625 - val_loss: 0.0338\n",
      "Epoch 2/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0301 - val_loss: 0.0243\n",
      "Epoch 3/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0240 - val_loss: 0.0209\n",
      "Epoch 4/100\n",
      "937/937 [==============================] - 122s 131ms/step - loss: 0.0212 - val_loss: 0.0192\n",
      "Epoch 5/100\n",
      "937/937 [==============================] - 122s 130ms/step - loss: 0.0196 - val_loss: 0.0182\n",
      "Epoch 6/100\n",
      "434/937 [============>.................] - ETA: 59s - loss: 0.0186"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_scores = []\n",
    "batch_size = 256\n",
    "\n",
    "for train, val in kfold.split(x_train, y_train):\n",
    "    model = init_model()\n",
    "    es = EarlyStopping(patience=3, verbose=1, restore_best_weights=True)\n",
    "    history = model.fit(x_train[train],\n",
    "                        y_train[train],\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_train[val], y_train[val]),\n",
    "                        callbacks=[es])\n",
    "    \n",
    "    y_pred_prob = model.predict(x_train[val], batch_size=batch_size, verbose=1)\n",
    "    y_pred = np.round(y_pred_prob)\n",
    "    \n",
    "    scores = {}\n",
    "    scores[\"accuracy\"] = accuracy_score(y_train[val], y_pred)\n",
    "    scores[\"F1 (macro)\"] = f1_score(y_train[val], y_pred, average=\"macro\")\n",
    "    scores[\"F1 (micro)\"] = f1_score(y_train[val], y_pred, average=\"micro\")\n",
    "    scores[\"LRAP\"] = label_ranking_average_precision_score(y_train[val],\n",
    "                                                           y_pred_prob)\n",
    "    scores[\"NDCG\"] = ndcg_score(y_train[val], y_pred_prob)\n",
    "    cv_scores.append(scores)\n",
    "    print(scores)\n",
    "    \n",
    "cv_scores_df = pd.DataFrame(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and their means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cv_scores_df)\n",
    "print(cv_scores_df.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
