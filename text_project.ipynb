{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random as rn\n",
    "import warnings\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import accuracy_score, f1_score, hamming_loss, average_precision_score, \\\n",
    "    ndcg_score, label_ranking_average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant, GlorotUniform\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, \\\n",
    "    GlobalMaxPooling1D, SpatialDropout1D, LSTM, GRU, Flatten, MaxPooling1D, \\\n",
    "    BatchNormalization, ReLU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import data\n",
    "import preprocessing\n",
    "\n",
    "seed = 42\n",
    "sns.set()\n",
    "\n",
    "def reset_seed():\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extracting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299773,)\n",
      "(299773, 126)\n",
      "Toronto stocks end higher after volatile session. CHANGE\t\t\t\t    CHANGE TSE\t  5900.37    +50.15   HI 5900.37\t    LO  5840.29 DJI\t  6611.05    +27.57   GOLD (LONDON)   US$350.00 +1.90 FTSE100    4248.10    -64.80   GOLD (NY-COMEX) US$354.80 +0.70 NIKKEI    17869.59   -133.81   LME CASH NICKEL US$7659   +99.0 CANDLR\t1.3883\t\t LME CASH ALUM   US$1602.0  -4.0 CAN 30-YR   107.41     -0.15   BRENT CRUDE     US$19.09  -0.27 --------------------MARKET COMMENT---------------------------- * Toronto stocks ended higher on Tuesday, buoyed by strength in golds and banking * Computer problems due to heavy trading in Bre-X Minerals hampered session * 84 million shares traded Toronto's key stock index ended higher on Tuesday as the saga of Bre-X Minerals Ltd and its Indonesian gold find continued to dominate Canada's biggest stock market. The TSE 300 Index climbed 50.15 points to close at 5900.37 in heavy turnover of 84.07 million shares worth C$1.4 billion. But the overall market was mixed with declining issues narrowly outpacing advances 476 to 464. 298 issues were flat. Frantic trading in Bre-X collapsed the TSE's computer trading system earlier in the day, forcing the exchange to halt trading in the stock before the market closed. Shares in the Calgary-based gold prospector were halted for a statement by the company this morning. When it resumed, a whopping 7.7 million shares changed hands in the first 22 minutes of trading before the system crashed. Bre-X closed up 1.35 at 3.85. It was the first time Bre-X traded since investors lopped nearly C$3 billion off its stock market value last Thursday. TSE officials said the trading problems were due to old technology which will be replaced. On the Montreal Exchange, Bre-X closed up 0.81 at 3.50 on 9.8 million shares. Analysts predicted more volatility for Bre-X shares this week. \"The question of what Bre-X will release over the next few days will be important to the market,\" said Josef Schachter, of Schachter Asset Management Inc. The gold sector rose nearly 136 points, leading 12 of 14 sub-indices higher. Other strong groups included financial services, consumer products, energy and transportation. The TSE posted minor losses in forestry and real estate. --- HOT STOCKS --- * Among bank shares, Bank of Nova Scotia rose 0.65 to 51.50 on 2.1 million shares, while Canadian Imperial Bank of Commerce added 0.50 to 31.80 on 2.1 million shares. ((Reuters Toronto Bureau (416) 941-8100))\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# data.extract_data(extraction_dir=\"train\", data_dir=\"data\", data_zip_name=\"reuters-training-corpus.zip\")\n",
    "\n",
    "train_df = pd.read_pickle(\"train/data.pkl\")\n",
    "\n",
    "# train_df = data.get_docs_labels(\"train/REUTERS_CORPUS_2\")\n",
    "# train_df.to_pickle(\"train/data.pkl\")\n",
    "\n",
    "train_docs = train_df[\"doc\"].values\n",
    "train_labels = np.array(train_df[\"labels\"].tolist())\n",
    "n_labels = len(data.CODEMAP)\n",
    "\n",
    "print(train_docs.shape)\n",
    "print(train_labels.shape)\n",
    "print(train_docs[2])\n",
    "print(train_labels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toronto stock end high volatile session change change tse 5900.37 +50.15 hi 5900.37 lo 5840.29 dji 6611.05 +27.57 gold london us$ 350.00 +1.90 ftse100 4248.10 -64.80 gold ny comex us$ 354.80 +0.70 nikkei 17869.59 -133.81 lme cash nickel us$ 7659 +99.0 candlr 1.3883 lme cash alum us$ 1602.0 -4.0 30-yr 107.41 -0.15 brent crude us$ 19.09 -0.27 --------------------market comment---------------------------- toronto stock end higher tuesday buoy strength gold banking computer problem heavy trading bre x minerals hamper session 84 million share trade toronto key stock index end higher tuesday saga bre x minerals ltd indonesian gold find continue dominate canada big stock market tse 300 index climb 50.15 point close 5900.37 heavy turnover 84.07 million share worth c$ 1.4 billion overall market mix decline issue narrowly outpace advance 476 464 298 issue flat frantic trading bre x collapse tse computer trading system earlier day force exchange halt trading stock market close share calgary base gold prospector halt statement company morning resume whopping 7.7 million share change hand 22 minute trading system crash bre x close 1.35 3.85 time bre x trade investor lop nearly c$ 3 billion stock market value thursday tse official say trading problem old technology replace montreal exchange bre x close 0.81 3.50 9.8 million share analyst predict volatility bre x share week question bre x release day important market say josef schachter schachter asset management inc. gold sector rise nearly 136 point lead 12 14 sub index high strong group include financial service consumer product energy transportation tse post minor loss forestry real estate hot stock bank share bank nova scotia rise 0.65 51.50 2.1 million share canadian imperial bank commerce add 0.50 31.80 2.1 million share reuters toronto bureau 416 941 8100\n"
     ]
    }
   ],
   "source": [
    "with open(\"train/preprocessed_docs_lemmatized_no_sw.pkl\", \"rb\") as f:\n",
    "    preprocessed_train_docs = pickle.load(f)\n",
    "\n",
    "# preprocessed_train_docs = preprocessing.preprocess_corpus(train_docs)\n",
    "# with open(\"train/preprocessed_docs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(preprocessed_train_docs, f)\n",
    "\n",
    "print(preprocessed_train_docs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = preprocessed_train_docs # choose between original and preprocessed versions\n",
    "n_vocabulary = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As token index sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sequence = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=n_vocabulary, filters=\"\")\n",
    "tokenizer.fit_on_texts(docs)\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "if n_vocabulary is None:\n",
    "    n_vocabulary = len(word_idx)\n",
    "\n",
    "print(n_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "(299773, 64)\n",
      "[1610   24   17   19 2090  436  114  114 4443  240  141  492  240 2564\n",
      " 2406  492 2264 1386  263 1863  492 1386  263  492  914  407  492 1610\n",
      "   24   17  700   37 3599  827  240  567  674  327  617  162 2578 1520\n",
      " 4945 4521  436 3271    5    7   20 1610  328   24   75   17  700   37\n",
      " 2578 1520 4945  151 1456  240  314  112]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-94f4c9509553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Document lengths\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "sequence_lengths = [len(s) for s in sequences]\n",
    "if n_sequence is None:\n",
    "    n_sequence = max(sequence_lengths)\n",
    "sequences = pad_sequences(sequences, maxlen=n_sequence, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "print(n_sequence)\n",
    "print(sequences.shape)\n",
    "print(sequences[2])\n",
    "\n",
    "sns.distplot(sequence_lengths)\n",
    "plt.title(\"Document lengths\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_term_matrix = tokenizer.texts_to_matrix(docs, mode=\"tfidf\")\n",
    "\n",
    "# print(doc_term_matrix.shape)\n",
    "# print(doc_term_matrix[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_embedding = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160759\n"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "\n",
    "window = 5\n",
    "w2v_path = f\"data/w2v_{window}_{n_embedding}.model\"\n",
    "\n",
    "try:\n",
    "    embedding_model = Word2Vec.load(w2v_path)\n",
    "except:\n",
    "    embedding_model = Word2Vec(sentences=[s.split() for s in docs],\n",
    "                               size=n_embedding, \n",
    "                               window=window,\n",
    "                               min_count=5,\n",
    "                               sg=1,\n",
    "                               workers=cpu_count(),\n",
    "                               seed=seed)\n",
    "    embedding_model.save(w2v_path)\n",
    "\n",
    "print(len(list(embedding_model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 256)\n",
      "[ 0.17539258 -0.06530272  0.01657451 -0.04425292  0.04128651 -0.18300489\n",
      " -0.16919801  0.0480488  -0.09133328  0.02587506 -0.01967425  0.09945878\n",
      " -0.05600481 -0.01685121 -0.03599447 -0.14082797 -0.01086318 -0.07289494\n",
      "  0.0402003  -0.04027929  0.18171482 -0.04842717  0.03684806 -0.0510481\n",
      "  0.20940183 -0.09696906 -0.08415043  0.0408814  -0.1309191  -0.13459311\n",
      "  0.46689954 -0.18623862  0.05366851 -0.05996224 -0.13319956 -0.24100822\n",
      "  0.05342331  0.14736585 -0.25805563  0.31074879 -0.32781357 -0.07095104\n",
      " -0.28958547  0.1536327   0.02000534 -0.08507846  0.00419282  0.01348357\n",
      "  0.06239876  0.19682285  0.19862361  0.06192612 -0.26880991  0.2868703\n",
      " -0.0284104   0.15227671 -0.22509414 -0.03397212  0.19979669 -0.04593811\n",
      "  0.1099792   0.01548944  0.16921869 -0.05802288 -0.32499835 -0.22160961\n",
      "  0.05431951 -0.05393629 -0.12470939 -0.03527912 -0.13933805 -0.10993122\n",
      " -0.01807112  0.18888257  0.37824026 -0.01249813 -0.03216061  0.16550902\n",
      " -0.11490405 -0.01706733 -0.0514022   0.10116427 -0.130915    0.19338837\n",
      "  0.24273583 -0.00947426  0.30365273  0.03064616  0.18221951 -0.04491771\n",
      " -0.21273483 -0.24777821  0.02345441  0.05513225  0.1793272   0.03420166\n",
      " -0.18700144 -0.16802149 -0.28900376  0.0134374   0.03601675 -0.01163298\n",
      "  0.24012862  0.38268062  0.29534483 -0.27307516  0.04336878 -0.20751141\n",
      "  0.15435676 -0.08344068 -0.10993855 -0.17020415  0.06080034 -0.19585659\n",
      " -0.06725669  0.10815314  0.18448977 -0.07454921 -0.03538286  0.10004397\n",
      "  0.02752361  0.10776202  0.27716571 -0.05569452 -0.22527045 -0.02368113\n",
      "  0.09049397  0.0362593   0.19314586  0.08720578 -0.20488213 -0.20601141\n",
      " -0.10168479  0.17306291 -0.15095626  0.20141339 -0.13031159 -0.2930311\n",
      " -0.10822594  0.13211681  0.12574936  0.15788476  0.03777872  0.05430784\n",
      " -0.02028085  0.08589081 -0.21433343 -0.29437518  0.38365856  0.01160286\n",
      " -0.22568278 -0.14324558 -0.27164212  0.04557002 -0.13594957 -0.07154472\n",
      "  0.25594142  0.27295801  0.23213175 -0.03170679  0.11953311  0.22423805\n",
      " -0.01673799 -0.05699946 -0.01785685  0.24200006  0.12270408  0.17886715\n",
      "  0.0582417   0.14881539  0.01145595  0.04585166  0.09956992 -0.07195296\n",
      " -0.06057838 -0.08374508 -0.01611461  0.24108276  0.12190975  0.15812506\n",
      " -0.19581009 -0.05357343 -0.1043957   0.04154731  0.05999554  0.16421185\n",
      " -0.00090712  0.05096835  0.07182094  0.23206605 -0.3490572  -0.15049285\n",
      " -0.16728461 -0.05582473  0.14193003  0.20580016 -0.0392748   0.09812338\n",
      " -0.26545608 -0.07239714  0.04259603  0.14496258  0.04230931 -0.23860203\n",
      " -0.03784676  0.02618643 -0.2071041   0.27838406  0.1649868  -0.09336159\n",
      " -0.0873594  -0.15951253  0.19413736  0.12089252  0.21618286 -0.16223127\n",
      " -0.07440235  0.02845805  0.21510532  0.17645378  0.11504612 -0.02777657\n",
      " -0.04655509  0.13731882 -0.23620516  0.08973085 -0.0328036  -0.23520076\n",
      " -0.08269925  0.15170757  0.16480862 -0.0363976  -0.14947772  0.23924245\n",
      " -0.22964537  0.06673072 -0.47210285  0.41494066  0.24518292 -0.02160032\n",
      "  0.06904915  0.14805193 -0.08515078  0.04349629  0.32536653 -0.02567887\n",
      " -0.19432268 -0.05296799  0.1826788  -0.12634988 -0.14461832  0.10669609\n",
      "  0.00451408  0.14421134  0.16422415  0.35485816]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((n_vocabulary, n_embedding))\n",
    "for token, i in word_idx.items():\n",
    "    if i >= n_vocabulary:\n",
    "        continue\n",
    "    if token in embedding_model:\n",
    "        embedding_matrix[i] = embedding_model[token]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.zeros(n_embedding)\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 64, 256)           1280000   \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 60, 64)            81984     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 60, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 56, 128)           41088     \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 56, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 7168)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               917632    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 126)               16254     \n",
      "=================================================================\n",
      "Total params: 2,354,494\n",
      "Trainable params: 1,073,982\n",
      "Non-trainable params: 1,280,512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def init_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "#     model.add(Embedding(input_dim=n_vocabulary, output_dim=n_embedding, input_length=n_sequence))\n",
    "    model.add(Embedding(\n",
    "        input_dim=n_vocabulary,\n",
    "        output_dim=n_embedding,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        input_length=n_sequence,\n",
    "        trainable=False\n",
    "    ))\n",
    "\n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Conv1D(64, 5, activation=\"relu\"))\n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(Dropout(.25))\n",
    "    \n",
    "#     model.add(GRU(128, dropout=.2))\n",
    "\n",
    "#     model.add(Dropout(.25))\n",
    "#     model.add(Conv1D(64, 5, activation=\"relu\"))\n",
    "#     model.add(Dropout(.25))\n",
    "#     model.add(Conv1D(128, 5, activation=\"relu\"))\n",
    "#     model.add(Dropout(.25))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(128))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(ReLU())\n",
    "#     model.add(Dropout(.25))\n",
    "#     model.add(Dense(128))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(ReLU())\n",
    "#     model.add(Dropout(.25))\n",
    "\n",
    "#     model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "#     model.add(Bidirectional(LSTM(128)))\n",
    "#     model.add(Dense(128, activation=\"relu\"))\n",
    "#     model.add(Dropout(.5))\n",
    "\n",
    "#     model.add(Dense(512, activation=\"relu\", input_shape=(n_vocabulary,)))\n",
    "#     model.add(Dropout(.5))\n",
    "\n",
    "#     model.add(Conv1D(100, 4, activation=\"relu\"))\n",
    "#     model.add(MaxPooling1D(pool_size=3))\n",
    "#     model.add(Conv1D(100, 2, activation=\"relu\"))\n",
    "#     model.add(Dropout(.5))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(300, activation=\"relu\"))\n",
    "    \n",
    "    model.add(Dense(n_labels, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "init_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = None\n",
    "x_train, y_train = shuffle(sequences, train_labels, random_state=seed, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0490 - val_loss: 0.0246\n",
      "Epoch 2/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0261 - val_loss: 0.0211\n",
      "Epoch 3/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0238 - val_loss: 0.0199\n",
      "Epoch 4/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0227 - val_loss: 0.0196\n",
      "Epoch 5/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0219 - val_loss: 0.0190\n",
      "Epoch 6/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0214 - val_loss: 0.0186\n",
      "Epoch 7/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0210 - val_loss: 0.0185\n",
      "Epoch 8/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0207 - val_loss: 0.0185\n",
      "Epoch 9/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0205 - val_loss: 0.0182\n",
      "Epoch 10/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0203 - val_loss: 0.0182\n",
      "Epoch 11/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0200 - val_loss: 0.0180\n",
      "Epoch 12/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0199 - val_loss: 0.0183\n",
      "Epoch 13/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0197 - val_loss: 0.0178\n",
      "Epoch 14/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0196 - val_loss: 0.0178\n",
      "Epoch 15/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0194 - val_loss: 0.0178\n",
      "Epoch 16/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0193 - val_loss: 0.0178\n",
      "Epoch 17/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0192 - val_loss: 0.0177\n",
      "Epoch 18/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0191 - val_loss: 0.0176\n",
      "Epoch 19/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0190 - val_loss: 0.0177\n",
      "Epoch 20/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0190 - val_loss: 0.0177\n",
      "Epoch 21/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0188 - val_loss: 0.0176\n",
      "Epoch 22/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0188 - val_loss: 0.0176\n",
      "Epoch 23/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0187 - val_loss: 0.0176\n",
      "Epoch 24/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0186 - val_loss: 0.0175\n",
      "Epoch 25/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0186 - val_loss: 0.0176\n",
      "Epoch 26/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0186 - val_loss: 0.0177\n",
      "Epoch 27/100\n",
      "1870/1874 [============================>.] - ETA: 0s - loss: 0.0185Restoring model weights from the end of the best epoch.\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0185 - val_loss: 0.0176\n",
      "Epoch 00027: early stopping\n",
      "469/469 [==============================] - 2s 4ms/step\n",
      "{'accuracy': 0.6664164790259361, 'F1 (macro)': 0.518672558603921, 'F1 (micro)': 0.87664312010363, 'LRAP': 0.9472327326667528, 'NDCG': 0.969944682792248}\n",
      "Epoch 1/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0492 - val_loss: 0.0245\n",
      "Epoch 2/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0262 - val_loss: 0.0217\n",
      "Epoch 3/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0239 - val_loss: 0.0201\n",
      "Epoch 4/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0227 - val_loss: 0.0196\n",
      "Epoch 5/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0220 - val_loss: 0.0189\n",
      "Epoch 6/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0215 - val_loss: 0.0187\n",
      "Epoch 7/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0211 - val_loss: 0.0186\n",
      "Epoch 8/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0208 - val_loss: 0.0185\n",
      "Epoch 9/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0205 - val_loss: 0.0188\n",
      "Epoch 10/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0203 - val_loss: 0.0182\n",
      "Epoch 11/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0200 - val_loss: 0.0180\n",
      "Epoch 12/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0198 - val_loss: 0.0182\n",
      "Epoch 13/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0197 - val_loss: 0.0179\n",
      "Epoch 14/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0196 - val_loss: 0.0178\n",
      "Epoch 15/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0195 - val_loss: 0.0179\n",
      "Epoch 16/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0193 - val_loss: 0.0179\n",
      "Epoch 17/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0192 - val_loss: 0.0178\n",
      "Epoch 18/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0192 - val_loss: 0.0179\n",
      "Epoch 19/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0190 - val_loss: 0.0178\n",
      "Epoch 20/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0189 - val_loss: 0.0178\n",
      "Epoch 21/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0189 - val_loss: 0.0177\n",
      "Epoch 22/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0188 - val_loss: 0.0178\n",
      "Epoch 23/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0187 - val_loss: 0.0177\n",
      "Epoch 24/100\n",
      "1871/1874 [============================>.] - ETA: 0s - loss: 0.0187Restoring model weights from the end of the best epoch.\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0187 - val_loss: 0.0177\n",
      "Epoch 00024: early stopping\n",
      "469/469 [==============================] - 2s 4ms/step\n",
      "{'accuracy': 0.6642481861396047, 'F1 (macro)': 0.5181262993319732, 'F1 (micro)': 0.8759285737322482, 'LRAP': 0.9460460347487761, 'NDCG': 0.9691631960390721}\n",
      "Epoch 1/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0483 - val_loss: 0.0240\n",
      "Epoch 2/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0260 - val_loss: 0.0208\n",
      "Epoch 3/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0238 - val_loss: 0.0196\n",
      "Epoch 4/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0227 - val_loss: 0.0191\n",
      "Epoch 5/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0220 - val_loss: 0.0189\n",
      "Epoch 6/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0214 - val_loss: 0.0185\n",
      "Epoch 7/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0210 - val_loss: 0.0182\n",
      "Epoch 8/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0207 - val_loss: 0.0184\n",
      "Epoch 9/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0205 - val_loss: 0.0181\n",
      "Epoch 10/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0202 - val_loss: 0.0180\n",
      "Epoch 11/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0201 - val_loss: 0.0179\n",
      "Epoch 12/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0199 - val_loss: 0.0177\n",
      "Epoch 13/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0198 - val_loss: 0.0177\n",
      "Epoch 14/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0196 - val_loss: 0.0176\n",
      "Epoch 15/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0195 - val_loss: 0.0177\n",
      "Epoch 16/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0194 - val_loss: 0.0176\n",
      "Epoch 17/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0193 - val_loss: 0.0175\n",
      "Epoch 18/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0192 - val_loss: 0.0175\n",
      "Epoch 19/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0190 - val_loss: 0.0176\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1869/1874 [============================>.] - ETA: 0s - loss: 0.0190Restoring model weights from the end of the best epoch.\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0190 - val_loss: 0.0175\n",
      "Epoch 00020: early stopping\n",
      "469/469 [==============================] - 2s 4ms/step\n",
      "{'accuracy': 0.6645817696605788, 'F1 (macro)': 0.5177131605124335, 'F1 (micro)': 0.8759829500514251, 'LRAP': 0.9470441251769898, 'NDCG': 0.9699883856895896}\n",
      "Epoch 1/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0492 - val_loss: 0.0242\n",
      "Epoch 2/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0261 - val_loss: 0.0210\n",
      "Epoch 3/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0238 - val_loss: 0.0200\n",
      "Epoch 4/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0226 - val_loss: 0.0194\n",
      "Epoch 5/100\n",
      "1874/1874 [==============================] - 20s 11ms/step - loss: 0.0219 - val_loss: 0.0189\n",
      "Epoch 6/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0214 - val_loss: 0.0186\n",
      "Epoch 7/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0210 - val_loss: 0.0183\n",
      "Epoch 8/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0207 - val_loss: 0.0187\n",
      "Epoch 9/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0205 - val_loss: 0.0181\n",
      "Epoch 10/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0202 - val_loss: 0.0181\n",
      "Epoch 11/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0201 - val_loss: 0.0179\n",
      "Epoch 12/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0199 - val_loss: 0.0180\n",
      "Epoch 13/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0197 - val_loss: 0.0180\n",
      "Epoch 14/100\n",
      "1873/1874 [============================>.] - ETA: 0s - loss: 0.0196Restoring model weights from the end of the best epoch.\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0196 - val_loss: 0.0179\n",
      "Epoch 00014: early stopping\n",
      "469/469 [==============================] - 2s 4ms/step\n",
      "{'accuracy': 0.658938519531641, 'F1 (macro)': 0.5087649883225681, 'F1 (micro)': 0.872640732166542, 'LRAP': 0.9445213470274861, 'NDCG': 0.9684954381799842}\n",
      "Epoch 1/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0494 - val_loss: 0.0242\n",
      "Epoch 2/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0261 - val_loss: 0.0214\n",
      "Epoch 3/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0238 - val_loss: 0.0204\n",
      "Epoch 4/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0227 - val_loss: 0.0194\n",
      "Epoch 5/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0219 - val_loss: 0.0193\n",
      "Epoch 6/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0214 - val_loss: 0.0189\n",
      "Epoch 7/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0210 - val_loss: 0.0187\n",
      "Epoch 8/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0207 - val_loss: 0.0187\n",
      "Epoch 9/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0204 - val_loss: 0.0185\n",
      "Epoch 10/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0202 - val_loss: 0.0183\n",
      "Epoch 11/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0200 - val_loss: 0.0181\n",
      "Epoch 12/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0198 - val_loss: 0.0181\n",
      "Epoch 13/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0196 - val_loss: 0.0182\n",
      "Epoch 14/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0195 - val_loss: 0.0181\n",
      "Epoch 15/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0194 - val_loss: 0.0180\n",
      "Epoch 16/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0193 - val_loss: 0.0180\n",
      "Epoch 17/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0192 - val_loss: 0.0180\n",
      "Epoch 18/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0191 - val_loss: 0.0179\n",
      "Epoch 19/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0190 - val_loss: 0.0178\n",
      "Epoch 20/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0189 - val_loss: 0.0180\n",
      "Epoch 21/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0188 - val_loss: 0.0177\n",
      "Epoch 22/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0187 - val_loss: 0.0181\n",
      "Epoch 23/100\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0187 - val_loss: 0.0179\n",
      "Epoch 24/100\n",
      "1869/1874 [============================>.] - ETA: 0s - loss: 0.0186Restoring model weights from the end of the best epoch.\n",
      "1874/1874 [==============================] - 19s 10ms/step - loss: 0.0186 - val_loss: 0.0178\n",
      "Epoch 00024: early stopping\n",
      "469/469 [==============================] - 2s 4ms/step\n",
      "{'accuracy': 0.6617239883910998, 'F1 (macro)': 0.5260658069024204, 'F1 (micro)': 0.8763356001305505, 'LRAP': 0.9461341478903448, 'NDCG': 0.9695783063999337}\n"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_scores = []\n",
    "batch_size = 128\n",
    "\n",
    "for train, val in kfold.split(x_train, y_train):\n",
    "    model = init_model()\n",
    "    es = EarlyStopping(patience=3, verbose=1, restore_best_weights=True)\n",
    "    history = model.fit(x_train[train],\n",
    "                        y_train[train],\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=100,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_train[val], y_train[val]),\n",
    "                        callbacks=[es])\n",
    "    \n",
    "    y_pred_prob = model.predict(x_train[val], batch_size=batch_size, verbose=1)\n",
    "    y_pred = np.round(y_pred_prob)\n",
    "    \n",
    "    scores = {}\n",
    "    scores[\"accuracy\"] = accuracy_score(y_train[val], y_pred)\n",
    "    scores[\"F1 (macro)\"] = f1_score(y_train[val], y_pred, average=\"macro\")\n",
    "    scores[\"F1 (micro)\"] = f1_score(y_train[val], y_pred, average=\"micro\")\n",
    "    scores[\"LRAP\"] = label_ranking_average_precision_score(y_train[val], y_pred_prob)\n",
    "    scores[\"NDCG\"] = ndcg_score(y_train[val], y_pred_prob)\n",
    "    cv_scores.append(scores)\n",
    "    print(scores)\n",
    "    \n",
    "cv_scores_df = pd.DataFrame(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and their means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>F1 (macro)</th>\n",
       "      <th>F1 (micro)</th>\n",
       "      <th>LRAP</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666416</td>\n",
       "      <td>0.518673</td>\n",
       "      <td>0.876643</td>\n",
       "      <td>0.947233</td>\n",
       "      <td>0.969945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.664248</td>\n",
       "      <td>0.518126</td>\n",
       "      <td>0.875929</td>\n",
       "      <td>0.946046</td>\n",
       "      <td>0.969163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.664582</td>\n",
       "      <td>0.517713</td>\n",
       "      <td>0.875983</td>\n",
       "      <td>0.947044</td>\n",
       "      <td>0.969988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.658939</td>\n",
       "      <td>0.508765</td>\n",
       "      <td>0.872641</td>\n",
       "      <td>0.944521</td>\n",
       "      <td>0.968495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.661724</td>\n",
       "      <td>0.526066</td>\n",
       "      <td>0.876336</td>\n",
       "      <td>0.946134</td>\n",
       "      <td>0.969578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  F1 (macro)  F1 (micro)      LRAP      NDCG\n",
       "0  0.666416    0.518673    0.876643  0.947233  0.969945\n",
       "1  0.664248    0.518126    0.875929  0.946046  0.969163\n",
       "2  0.664582    0.517713    0.875983  0.947044  0.969988\n",
       "3  0.658939    0.508765    0.872641  0.944521  0.968495\n",
       "4  0.661724    0.526066    0.876336  0.946134  0.969578"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy      0.663182\n",
      "F1 (macro)    0.517869\n",
      "F1 (micro)    0.875506\n",
      "LRAP          0.946196\n",
      "NDCG          0.969434\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "display(cv_scores_df)\n",
    "print(cv_scores_df.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
